Probabilistic bug hunting
Basic statistics for programmers
2011-09-07

%!postproc: '!!r1w3_n4_results!!' <iframe src="r1w3_n4_results.html" width="100%" height="500"></iframe>


Have you ever run into a bug that, no matter how careful you are trying to
reproduce it, it only happens sometimes? And then, you think you've got it, and
finally solved it - and tested a couple of times without any manifestation. How
do you know that you have tested enough? Are you sure you were not "lucky" in
your tests?

In this article we will see how those questions can be answered and the math
behind it.



= The Bug =

The following program is supposed to generate a random 16-bit integer and print
it on stdout.

%!include: ``hasbug.c``

On my architecture (Linux on IA-32) it has a bug that makes it print "error"
instead of the number sometimes.



= The Model =

Every time we run the program, the bug can either show up or not. We will model
a single program run as a
[bernoulli trial http://en.wikipedia.org/wiki/Bernoulli_trial], with success
defined as "seeing the bug", as that is the event we are interested in. As a
bernoulli trial, the number of errors \(k\) of running the program \(n\) times
follows a
[binomial distribution http://en.wikipedia.org/wiki/Binomial_distribution]
\(k \sim B(n,p)\), where \(p\) is the probability of seeing the bug in a single run.

By using this model we are implicitly assuming that all our tests are performed
under the same conditions. In order words: if the bug happens more ofter in one
environment, we either test always in that environment or never; if the bug
gets more and more frequent the longer the PC is running, we reset the PC after
each trial. The idea is to test in such a way that the parameter \(p\) is
constant. Figuring out a single value for a parameter that keeps changing makes
no sense, and can lead us to wrong conclusions.

This model is the same we would use if we were taking out balls from a box with
green and red balls. Instead of running a program, we take a ball off the box
and look at its color: if it is
red, we have seen the bug; if it is green, we have not. \(p\) is the number of
red balls in the box divided by the number of green balls. The total number of
balls in the box is not relevant to our analogy, only the red/green ratio. After
seeing the ball, we put it back in the box, to keep \(p\) constant.

 [boxballs.png] 



= Estimating \(p\) =

Before we try fixing anything, we have to know more about the bug, starting by
the probability \(p\) of reproducing it. We can estimate this probability by
dividing the number of times we see the bug \(k\) by the number of times we
tested for it \(n\). Let's try that with our sample bug:

```
$ ./hasbug
831
$ ./hasbug
-28708
$ ./hasbug
error
$ ./hasbug
31774
```

So: \(k=1, n=4 \Rightarrow p \sim 0.25\), right? It would be better if we tested
more, but how much more, and exactly what would be better?



== \(p\) precision ==

Let's go back to our box analogy: imagine that there are 4 balls in the box, one
red and three green. That means \(p = 1/4\). What are the possible results when
we test four times?

|| Red balls | Green balls | \(p\) estimate |
| 0          | 4           | 0              |
| 1          | 3           | 0.25           |
| 2          | 2           | 0.5            |
| 3          | 1           | 0.75           |
| 4          | 0           | 1              |

The less we test, the smaller is our precision. Roughly, \(p\) precision will
be at most \(1/n\). That means that we have a precision of \(\frac{1}{1000}\)
when we test \(1000\) times.



== \(p\) probability ==

Let's now approach the problem from another angle: if \(p = 1/4\), what are the
odds of seeing one error in four tests? Let's name the 4 balls as 0-red,
1-green, 2-green and 3-green:

!!r1w3_n4_results!!

The table above has all the possible results for getting 4 balls out of the
box. That's \(4^4=256\) rows, generated by [this $cwd$/box] python script.
The same script counts the number of red balls in each row, and outputs the
following table:

%!include: r1w3_n4_probabilities.t2t

That means that, for \(p=1/4\), we see 1 red ball and 3 green balls only 42% of
the time when getting out 4 balls.

What if \(p = 1/3\) - one red ball and two green balls? We would get the
following table:

%!include: r1w2_n4_probabilities.t2t

What about \(p = 1/2\)?

%!include: r1w1_n4_probabilities.t2t

So, you've seen the bug once in 4 trials. What is the value of \(p\)? You know
that can happen 42% of the time if \(p=1/4\), but you also know it can happen
39% of the time if \(p=1/3\). The graph bellow shows the discrete probability
for all \(p\) percentual values for getting 1 red and 3 green balls:

 [r1w3_dist.png] 


The fact is that, //given the data//, \(p\)
follows a [beta distribution http://en.wikipedia.org/wiki/Beta_distribution]
\(Beta(k+1, n-k+1) = Beta(2, 4)\)
([1 http://stats.stackexchange.com/questions/13225/what-is-the-distribution-of-the-binomial-distribution-parameter-p-given-a-sampl])
The graph below shows the probability distribution density of \(p\):

 [r1w3_dens.png] 

The R script used to generate the first plot is [here $cwd$/pdistplot.R], the
one used for the second plot is [here $cwd$/pdensplot.R].



== Increasing \(n\), narrowing down the confidence interval ==

What happens when we test more? We obviously increase our precision, as it is at
most \(1/n\), as we said before - there is no way to estimate that \(p=1/3\) when we
only test twice. But, there is also another effect: the distribution for \(p\)
gets taller and narrower around the observed ratio \(k/n\):

 [pdens_many.png] 

This means that we have a higher probability that \(p\) is within a given
interval, **or** that we have a narrower interval for a given probability.
The usually take the second approach: fix the probability, now called
[confidence level http://en.wikipedia.org/wiki/Confidence_level] and narrow down
the [confidence interval http://en.wikipedia.org/wiki/Confidence_interval] -
i.e. the interval estimated for \(p\) - as we test.

The typical confidence level is 95%. This means that if we repeated our test a
huge number of times, in at least 95% of the tests we would measure \(p\) inside
the confidence interval. This implies that we are rejecting 5% of our results
because they are considered unlikely.

Why don't we take a confidence level of 100% and be sure about the interval?
Well, you'll notice that the density function is only 0 at 0 and 1. That means
that a 100% confidence interval is the whole open range \((0, 1)\), no matter
how many times we test. That would lead us nowhere.



= Is the bug fixed? =

The code has been changed in a way that might fix the bug, but we are not sure.
How many times should we test it, and how sure can we get?

For any value of \(p\) there is always the chance that we don't reproduce the
bug, for any number of times that we test. Once again, we can take one of two
approaches: we either test a number of times and state the probability
\(\alpha\) of making a
[Type I error http://en.wikipedia.org/wiki/Type_1_error#Type_I_error], or we
keep \(\alpha\) constant (typically at 5%) and test enough times to guarantee
it.



