Probabilistic bug hunting
Basic statistics for programmers
2011-09-07

%!postproc: '!!hasbug_results!!' <iframe src="hasbug_results.html" width="100%" height="500"></iframe>


Have you ever run into a bug that, no matter how precise you are trying to
reproduce it, it only happens sometimes? And then, you think you've got it and
finally solved it, and tested a couple of times without any manifestation. How
do you know that you tested enough? Are you sure you were not "lucky" in your
tests?

In this article we will see how those questions can be answered and the math
behind it.



= The Bug =

The following program is supposed to generate a random 16-bit integer and print
it on stdout.

%!include: ``hasbug.c``

On my architecture (Linux on IA-32) it has a bug that makes it print "error"
instead of the number sometimes.



= The Model =

Every time we run the program, the bug can either show up or not. We will model
a single program run as a
[bernoulli trial http://en.wikipedia.org/wiki/Bernoulli_trial], with success
defined as "seeing the bug", as that is the event we are testing for. As a
bernoulli trial, the model for the results of running the program \(n\) times
follows a
[binomial distribution http://en.wikipedia.org/wiki/Binomial_distribution]
\(B(n,p)\), where \(p\) is the probability of seeing the bug in a single run.

By using this model we are implicitly assuming that all our tests are performed
under the same conditions. In order words: if the bug happens more ofter in one
environment, we either test always in that environment or never; if the bug
gets more and more frequent the longer the PC is running, we reset the PC after
each trial. The idea is to test in such a way that the parameter \(p\) is
constant. If we ignore this requirement and mix tests, misusing our model, we
can be lead to false conclusions.

The model is analogous to a box with red and white balls. Each trial
is equivalent to taking a ball off the box and looking at its color: if it is
red, we have seen the bug; if it is white, we have not. \(p\) is the number of
red balls in the box divided by the number of white balls. The total number of
balls in the box is not relevant to our analogy, only the red/white ratio. After
seeing the ball, we put it back in the box, as \(p\) must be remain constant in
all trials.




= Estimating \(p\) =

Before we try fixing anything, we have to know more about the bug, starting by
the probability \(p\) of reproducing it. We can estimate this probability by
dividing the number of times we see the bug \(k\) by the number of times we
tested for it \(n\). Let's try that with our sample bug:

```
$ ./hasbug
831
$ ./hasbug
-28708
$ ./hasbug
error
$ ./hasbug
31774
```

So: \(k=1, n=4 \Rightarrow p \sim 0.25\), right? It would be better if we tested
more, but how much more, and exactly what would be better?



== \(p\) precision ==

Let's go back to our box analogy: imagine that there are 4 balls in the box, one
red and three white. That means \(p = 1/4\). What are the possible results when
we test four times?

|| Red balls | White balls | \(p\) estimate |
| 0          | 4           | 0              |
| 1          | 3           | 0.25           |
| 2          | 2           | 0.5            |
| 3          | 1           | 0.75           |
| 4          | 0           | 1              |

The less we test, the smaller is our precisions. Roughly, \(p\) precision will
be at most \(1/n\). That means that we have a precision of 0.001 when we test 1000
times.



== \(p\) probability ==

Let's now approach the problem from another angle: if \(p = 1/4\), what are the
odds of seeing one error in four tests? Let's name the 4 balls as 0-red,
1-white, 2-white and 3-white:

!!hasbug_results!!

The table above has all the possible results for getting 4 balls out of the
box. That's \(4^4=256\) rows, generated by [this $cwd$/box] python script.
The same script counts the number of red balls in each row, and outputs the
following table:

%!csv: articles/bugprobhunt/hasbug_probabilities.csv


The same scripts tells me that there are 108 rows with a single red ball. That
means that the probability of seeing one red ball in 4 trials for \(p = 1/4\) is
\(108/256 = 42.1875%\).


but as the programmers we are, we will never type
that by hand; the program I wrote to generate. Of course I did not type that, the python program
used to generate it is [here buildtable].




We have 4/9 (~44%) probability of estimating \(p\) as 0.5 if it is 1/3 and we
test only twice. That's the same probability of estimating \(p\) as 0 in the
same conditions.



== Increasing \(n\) ==



From wikipedia, we can see that this can be calculated by
using the probability mass function of the binomial distribution:

 [http://upload.wikimedia.org/math/0/c/1/0c1ae7a35c20afa9f189dffa5d3c0c23.png] 

Where:

 [http://upload.wikimedia.org/math/c/2/d/c2d02458d8c35f11e465c639ba62f081.png] 

(we can use the function //dbinom// in R to calculate that)

By using those formulas, we can see that the probability of seeing one error in
two tests give that //p// is 0.2 is 0.32. That means that if we perform many
2-runs tests, we will see the bug once in 32% of the tests.

On the other hand, the probability of seeing 500 errors in 1000 runs for a //p//
of 0.2 is 3e-99. You would have to test more than once for
[each atom in the known universe http://en.wikipedia.org/wiki/Observable_universe#Matter_content]
on average to see that happen.

So, if we test twice, //p// might be 0.5, but there is a good chance that it
might be 0.2 also. 



