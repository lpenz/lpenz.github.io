Probabilistic bug hunting
Basic statistics for programmers
2011-09-07

%!postproc: '!!r1w3_n4_results!!' <iframe src="r1w3_n4_results.html" width="100%" height="500"></iframe>


Have you ever run into a bug that, no matter how careful you are trying to
reproduce it, it only happens sometimes? And then, you think you've got it, and
finally solved it - and tested a couple of times without any manifestation. How
do you know that you have tested enough? Are you sure you were not "lucky" in
your tests?

In this article we will see how those questions can be answered and the math
behind it.



= The Bug =[thebug]

The following program is supposed to generate two random 8-bit integer and print
them on stdout:

%!include: ``hasbug.c``

On my architecture (Linux on IA-32) it has a bug that makes it print "error"
instead of the numbers sometimes.



= The Model =

Every time we run the program, the bug can either show up or not. It has a
non-deterministic behaviour that requires statistical analysis.

We will model a single program run as a
[Bernoulli trial http://en.wikipedia.org/wiki/Bernoulli_trial], with success
defined as "seeing the bug", as that is the event we are interested in. We have
the following parameters when using this model:
- \(n\): the number of tests made;
- \(k\): the number of times the bug was observed in the \(n\) tests;
- \(p\): the unknown (and, most of the time, unknowable) probability of seeing
  the bug.


As a Bernoulli trial, the number of errors \(k\) of running the program \(n\)
times follows a
[binomial distribution http://en.wikipedia.org/wiki/Binomial_distribution]
\(k \sim B(n,p)\). We will use this model to estimate \(p\) and to validate the
hypotheses that the bug no longer exists, after fixing the bug in whichever
way we can.

By using this model we are implicitly assuming that all our tests are performed
independently and identically. In order words: if the bug happens more ofter in
one environment, we either test always in that environment or never; if the bug
gets more and more frequent the longer the computer is running, we reset the
computer after each trial. If we don't do that, we are effectively estimating
the value of \(p\) with trials from different experiments, while in truth each
experiment has it's own \(p\). We will find a single value anyway, but it has no
meaning and can lead us to wrong conclusions.



== Physical analogy ==

Another way of thinking about the model and the strategy is by creating a
physical analogy with a box that has an unknown number of green and red balls:
- Bernoulli trial: taking a single ball out of the box and looking at its
  color - if it is red, we have observed the bug, otherwise we haven't. We then
  put the ball back in the box.
- \(n\): the total number of trials we have performed.
- \(k\): the total number of red balls seen.
- \(p\): the total number of red balls in the box divided by the total number of
  green balls in the box.


Some things become clearer when we think about this analogy:
- If we open the box and count the balls, we can know \(p\), in contrast with
  our original problem.
- Without opening the box, we can estimate \(p\) by repeating the trial. As
  \(n\) increases, our estimate for \(p\) improves. Mathematically:
  \[p = \lim_{n\to\infty}\frac{k}{n}\]
- Performing the trials in different conditions is like taking balls out of
  several different boxes. The results tells us nothing about any single box.


 [boxballs.png] 



= Estimating \(p\) =

Before we try fixing anything, we have to know more about the bug, starting by
the probability \(p\) of reproducing it. We can estimate this probability by
dividing the number of times we see the bug \(k\) by the number of times we
tested for it \(n\). Let's try that with our sample bug:

```
$ ./hasbug
67 -68
$ ./hasbug
79 -101
$ ./hasbug
error
$ ./hasbug
88 -19
```

So: \(k=1, n=4 \Rightarrow p \sim 0.25\), right? It would be better if we tested
more, but how much more, and exactly what would be better?



== \(p\) precision ==

Let's go back to our box analogy: imagine that there are 4 balls in the box, one
red and three green. That means \(p = 1/4\). What are the possible results when
we test three times?

|| Red balls | Green balls | \(p\) estimate |
| 0          | 3           | 0%             |
| 1          | 2           | 33%            |
| 2          | 1           | 66%            |
| 3          | 0           | 100%           |

The less we test, the smaller our precision is. Roughly, \(p\) precision will
be at most \(1/n\) - in this case, 0.333. That's the step of values we can find
for \(p\), and the minimal value for it.

So, testing more improves the precision of our estimate.



== \(p\) likelihood ==

Let's now approach the problem from another angle: if \(p = 1/4\), what are the
odds of seeing one error in four tests? Let's name the 4 balls as 0-red,
1-green, 2-green and 3-green:

!!r1w3_n4_results!!

The table above has all the possible results for getting 4 balls out of the
box. That's \(4^4=256\) rows, generated by [this $cwd$/box] python script.
The same script counts the number of red balls in each row, and outputs the
following table:

%!include: r1w3_n4_probabilities.t2t

That means that, for \(p=1/4\), we see 1 red ball and 3 green balls only 42% of
the time when getting out 4 balls.

What if \(p = 1/3\) - one red ball and two green balls? We would get the
following table:

%!include: r1w2_n4_probabilities.t2t

What about \(p = 1/2\)?

%!include: r1w1_n4_probabilities.t2t

So, you've seen the bug once in 4 trials. What is the value of \(p\)? You know
that can happen 42% of the time if \(p=1/4\), but you also know it can happen
39% of the time if \(p=1/3\). The graph bellow shows the discrete likelihood
for all \(p\) percentual values for getting 1 red and 3 green balls:

 [r1w3_dist.png] 


The fact is that, //given the data//, the estimate for \(p\)
follows a [beta distribution http://en.wikipedia.org/wiki/Beta_distribution]
\(Beta(k+1, n-k+1) = Beta(2, 4)\)
([1 http://stats.stackexchange.com/questions/13225/what-is-the-distribution-of-the-binomial-distribution-parameter-p-given-a-sampl])
The graph below shows the probability distribution density of \(p\):

 [r1w3_dens.png] 

The R script used to generate the first plot is [here $cwd$/pdistplot.R], the
one used for the second plot is [here $cwd$/pdensplot.R].



== Increasing \(n\), narrowing down the interval ==

What happens when we test more? We obviously increase our precision, as it is at
most \(1/n\), as we said before - there is no way to estimate that \(p=1/3\) when we
only test twice. But there is also another effect: the distribution for \(p\)
gets taller and narrower around the observed ratio \(k/n\):

 [pdens_many.png] 



== Framework ==

So, which value will we use for \(p\)?
- The smaller the value of \(p\), the more we have to test to reach a given
  confidence in the bug solution.
- We must, then, choose the probability of error that we want to tolerate, and
  take the //smallest// value of \(p\) that we can. A usual value is 5%.
- That means that we take the value of \(p\) that leaves 5% of the area of the
  density curve out on the left side.
- That way, if the observed \(k/n\) remains somewhat constant, our estimate for
  \(p\) will raise, converging to the "real" value.
- As \(p\) raises, the amount of testing we have to do after fixing the bug
  decreases.


By using this framework we have direct, visual and tangible incentives to test
more.

In order to calculate the minimum \(p\) with the mentioned properties, we have
to solve the following equation (where \(\alpha\) is the error we can
tolerate):

\[\sum_{k=0}^{k}{n\choose{k}}p ^k(1-p)^{n-k}=\frac{\alpha}{2} \]

Fortunately, that's the formula for the confidence interval of the binomial
distribution, and there are a lot of sites that can calculate it:
- http://statpages.org/confint.html: \(\alpha\) here is fixed in 5%.
- http://www.danielsoper.com/statcalc3/calc.aspx?id=85: results for \(\alpha\)
  1%, 5% and 10%.
- https://www.google.com.br/search?q=binomial+confidence+interval+calculator:
  google search.


You want to take the smaller \(p\) value provided by those calculators.



= Is the bug fixed? =

So, you have tested a lot and estimated \(p\). The next step is fixing the bug.

After fixing the bug, you will want to test again, enough times so to be
confident that the bug is fixed. How much testing is enough testing?

Let's say that \(t\) is the number of times we test the bug after it is fixed.
Then, if our fix is not effective and the bug still presents itself with
the probability \(p\) that we calculated, the probability of //not// seeing the
bug after \(t\) tests is:

\[\alpha = (1-p)^t \]

Here, \(\alpha\) is also the probability of making a
[type I error http://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error],
while \(1 - \alpha\) is the statistical significance of our tests.

We now have two options:
- arbitrarily determining a standard statistical significance and testing enough
  times to assert it.
- test as much as we can and report the achieved statistical significance.


Both options are valid. The first one is not always feasible, if each trial
takes too much time and/or there is not enough time to make the required amount
of testing for the standard significance.

The standard statistical significance in the industry is 5%, we recommend either
that or less.



= Back to the Bug =



== Testing 20 times ==

I made a script that ran the program presented in [The Bug #thebug] and
registered the results ([data trials.csv], [script trialdoer]). The observed
\(k/n\) ration and the estimated \(p\) evolved as shown in the following graph:

 [trials20.png] 

After those 20 tests, our lower estimate for \(p\) is about 12%.

Suppose that we fix the bug and test it again. The following graph shows the
statistical significance corresponding to the number of tests we do:

 [after20.png] 

In words: we have to test 24 times after fixing the bug to reach 95% statistical
significance, and 35 to reach 99%.

Now, what happens if we test more before fixing the bug?



== Testing 5000 times ==

Let's now assume that we tested 5000 times before fixing the bug. The graph
bellow shows \(k/n\) and \(p\):

 [trials5000.png] 

After those 5000 tests, our lower estimate for \(p\) is about 23% - much closer
to the real \(p\).

The following graph shows the statistical significance corresponding to the
number of tests we do after fixing the bug:

 [after5000.png] 

We can see in that graph that after about 11 tests we reach 95%, and after about
16 we get to 99%. As we have tested more before fixing the bug, we found a
higher estimate for \(p\), and that allowed us to test less after fixing the
bug.



= Optimal testing =

We have seen that we decrease \(t\) as we increase \(n\), as that can
potentially increases our estimated \(p\). Of course, our estimated \(p\) can
decrease as we test, but that means that we "got lucky" in the first trials and
we are getting to know the bug better - the estimate is approaching the real
value in a non-deterministic way, after all.

But, how much should we test before fixing the bug? Which value is an optimal
value for \(n\)?

To define an optimal value for \(n\), we will minimize the sum \(n+t\). This
criteria gives us the benefit of minimizing the total amount of testing without
compromising our guarantees. Minimizing the testing can be fundamental if each
test costs significant time or resources.

The graph bellow shows us the evolution of the value of \(t\) and \(t+n\) using
the data we generated for our bug:

 [tbyn.png] 

We can see clearly that there are some low values of \(n\) and \(t\) that give
us the guarantees we need. Those values are \(n = 15\) and \(t = 24\), which
gives us \(t+n = 39\).

While you can use this technique to minimize the total number of tests performed
(even more so when testing is expensive), testing more is always a good thing,
as it always improves our guarantee, be it in \(n\) by providing us with a
better \(p\) or in \(t\) by increasing the significance of the test. So, test
until you see the bug at least once, and then at least the amount specified by
this technique - but also test more if you can, there is no upper bound.



= Conclusions =

When a programmer finds a bug that behaves in a non-deterministic way, he
knows he should test enough to know more about the bug, and then even more
after fixing it. In this article we have presented a framework that provides
criteria to define numerically how much testing is "enough" and "even more." The
same technique also provides a method to objectively measure the guarantee that
the amount of testing performed provides, when it is not possible to test
"enough."

We have also provided a real example (even though the bug itself is artificial)
where the framework is applied.

As usual, the source code of this page (R scripts, etc) can be found and
downloaded in https://github.com/lpenz/lpenz.org.



