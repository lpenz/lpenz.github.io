Probabilistic bug hunting
Basic statistics for programmers
2011-09-07

%!postproc: '!!r1w3_n4_results!!' <iframe src="r1w3_n4_results.html" width="100%" height="500"></iframe>


Have you ever run into a bug that, no matter how careful you are trying to
reproduce it, it only happens sometimes? And then, you think you've got it, and
finally solved it - and tested a couple of times without any manifestation. How
do you know that you have tested enough? Are you sure you were not "lucky" in
your tests?

In this article we will see how those questions can be answered and the math
behind it.



= The Bug =

The following program is supposed to generate two random 8-bit integer and print
them on stdout:

%!include: ``hasbug.c``

On my architecture (Linux on IA-32) it has a bug that makes it print "error"
instead of the numbers sometimes.



= The Model =

Every time we run the program, the bug can either show up or not. It has a
non-deterministic behaviour that requires statistical analysis.

We will model a single program run as a
[Bernoulli trial http://en.wikipedia.org/wiki/Bernoulli_trial], with success
defined as "seeing the bug", as that is the event we are interested in. We have
the following parameters when using this model:
- \(n\): the number of tests made;
- \(k\): the number of times the bug was observed in the \(n\) tests;
- \(p\): the unknown (and, most of the time, unknowable) probability of seeing
  the bug.


As a Bernoulli trial, the number of errors \(k\) of running the program \(n\)
times follows a
[binomial distribution http://en.wikipedia.org/wiki/Binomial_distribution]
\(k \sim B(n,p)\). We will use this model to estimate \(p\) and to validate the
hypotheses that the bug no longer exists, after fixing the bug in whichever
way we can.

By using this model we are implicitly assuming that all our tests are performed
independently and identically. In order words: if the bug happens more ofter in
one environment, we either test always in that environment or never; if the bug
gets more and more frequent the longer the computer is running, we reset the
computer after each trial. If we don't do that, we are effectively estimating
the value of \(p\) with trials from different experiments, while in truth each
experiment has it's own \(p\). We will find a single value anyway, but it has no
meaning and can lead us to wrong conclusions.



== Physical analogy ==

Another way of thinking about the model and the strategy is by creating a
physical analogy with a box that has an unknown number of green and red balls:
- Bernoulli trial: taking a single ball out of the box and looking at its
  color - if it is red, we have observed the bug, otherwise we haven't. We then
  put the ball back in the box.
- \(n\): the total number of trials we have performed.
- \(k\): the total number of red balls seen.
- \(p\): the total number of red balls in the box divided by the total number of
  green balls in the box.


Some things become clearer when we think about this analogy:
- If we open the box and count the balls, we can know \(p\), in contrast with
  our original problem.
- Without opening the box, we can estimate \(p\) by repeating the trial. As
  \(n\) increases, our estimate for \(p\) improves. Mathematically:
  \[p = \lim_{n\to\infty}\frac{k}{n}\]
- Performing the trials in different conditions is like taking balls out of
  several different boxes. The results tells us nothing about any single box.


 [boxballs.png] 



= Estimating \(p\) =

Before we try fixing anything, we have to know more about the bug, starting by
the probability \(p\) of reproducing it. We can estimate this probability by
dividing the number of times we see the bug \(k\) by the number of times we
tested for it \(n\). Let's try that with our sample bug:

```
$ ./hasbug
67 -68
$ ./hasbug
79 -101
$ ./hasbug
error
$ ./hasbug
88 -19
```

So: \(k=1, n=4 \Rightarrow p \sim 0.25\), right? It would be better if we tested
more, but how much more, and exactly what would be better?



== \(p\) precision ==

Let's go back to our box analogy: imagine that there are 4 balls in the box, one
red and three green. That means \(p = 1/4\). What are the possible results when
we test three times?

|| Red balls | Green balls | \(p\) estimate |
| 0          | 3           | 0%             |
| 1          | 2           | 33%            |
| 2          | 1           | 66%            |
| 3          | 0           | 100%           |

The less we test, the smaller our precision is. Roughly, \(p\) precision will
be at most \(1/n\) - in this case, 0.333. That's the step of values we can find
for \(p\), and the minimal value for it.

So, testing more improves the precision of our estimate.



== \(p\) likelihood ==

Let's now approach the problem from another angle: if \(p = 1/4\), what are the
odds of seeing one error in four tests? Let's name the 4 balls as 0-red,
1-green, 2-green and 3-green:

!!r1w3_n4_results!!

The table above has all the possible results for getting 4 balls out of the
box. That's \(4^4=256\) rows, generated by [this $cwd$/box] python script.
The same script counts the number of red balls in each row, and outputs the
following table:

%!include: r1w3_n4_probabilities.t2t

That means that, for \(p=1/4\), we see 1 red ball and 3 green balls only 42% of
the time when getting out 4 balls.

What if \(p = 1/3\) - one red ball and two green balls? We would get the
following table:

%!include: r1w2_n4_probabilities.t2t

What about \(p = 1/2\)?

%!include: r1w1_n4_probabilities.t2t

So, you've seen the bug once in 4 trials. What is the value of \(p\)? You know
that can happen 42% of the time if \(p=1/4\), but you also know it can happen
39% of the time if \(p=1/3\). The graph bellow shows the discrete likelihood
for all \(p\) percentual values for getting 1 red and 3 green balls:

 [r1w3_dist.png] 


The fact is that, //given the data//, the estimate for \(p\)
follows a [beta distribution http://en.wikipedia.org/wiki/Beta_distribution]
\(Beta(k+1, n-k+1) = Beta(2, 4)\)
([1 http://stats.stackexchange.com/questions/13225/what-is-the-distribution-of-the-binomial-distribution-parameter-p-given-a-sampl])
The graph below shows the distribution density of \(p\):

 [r1w3_dens.png] 

The R script used to generate the first plot is [here $cwd$/pdistplot.R], the
one used for the second plot is [here $cwd$/pdensplot.R].



== Increasing \(n\), narrowing down the confidence interval ==

What happens when we test more? We obviously increase our precision, as it is at
most \(1/n\), as we said before - there is no way to estimate that \(p=1/3\) when we
only test twice. But, there is also another effect: the distribution for \(p\)
gets taller and narrower around the observed ratio \(k/n\):

 [pdens_many.png] 

This means that we have a higher probability that \(p\) is within a given
interval, **or** that we have a narrower interval for a given probability.
The usually take the second approach: fix the probability, now called
[confidence level http://en.wikipedia.org/wiki/Confidence_level] and narrow down
the [confidence interval http://en.wikipedia.org/wiki/Confidence_interval] -
i.e. the interval estimated for \(p\) - as we test.

The typical confidence level is 95%. This means that if we repeated our test a
huge number of times, in at least 95% of the tests we would measure \(p\) inside
the confidence interval. This implies that we are rejecting 5% of our results
because they are considered unlikely.

Why don't we take a confidence level of 100% and be sure about the interval?
Well, you'll notice that the density function is only 0 at 0 and 1. That means
that a 100% confidence interval is the whole open range \((0, 1)\), no matter
how many times we test. That would lead us nowhere.



= Is the bug fixed? =

The code has been changed in a way that might fix the bug, but we are not sure.
How many times should we test it, and how sure can we get?

For any value of \(p\) there is always the chance that we don't reproduce the
bug, for any number of times that we test. Once again, we can take one of two
approaches: we either test a number of times and state the probability
\(\alpha\) of making a
[Type I error http://en.wikipedia.org/wiki/Type_1_error#Type_I_error], or we
keep \(\alpha\) constant (typically at 5%) and test enough times to guarantee
it.



