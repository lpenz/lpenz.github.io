Probabilistic bug hunting
An introduction to statistics
2011-09-07


Have you ever run into a bug that, no matter how precise you are trying to
reproduce it, it only happens sometimes? And then, you think you've got it and
finally solved it, and tested a couple of times without any manifestation. How
do you know that you tested enough? Are you sure you were not "lucky" in your
tests?

In this article we will see how those questions can be answered and the math
behind it.



= The Bug =

The following program is supposed to generate a random 16-bit integer and print
it on stdout.

%!include: ``hasbug.c``

On my architecture (Linux on IA-32) it has a bug that makes it print "error"
instead of the number sometimes. We will analyse the bug throughout the article.



= The Model =

Every time we run the program, the bug can either show up or not. We will model
a single program run as a
[bernoulli trial http://en.wikipedia.org/wiki/Bernoulli_trial], with success
defined as "seeing the bug", as that is the event we are testing for. As a
bernoulli trial, the model for the results of running the program //n// times
follows a
[binomial distribution http://en.wikipedia.org/wiki/Binomial_distribution]
//B(n,p)//, where //p// is the probability of seeing the bug in a single run.

By using this model we are implicitly assuming that all our tests are performed
under the same conditions. In order words: if the bug happens more ofter in one
environment, we either test always in that environment or never; if the bug
gets more and more frequent the longer the PC is running, we reset the PC after
each trial. The idea is to test in such a way that the parameter //p// is
constant. If we ignore this requirement and make mixed tests, our model won't
work, and can lead us to wrong conclusions.

The model is analogous to a box with red and white balls. Each trial
is equivalent to taking a ball off the box and looking at its color: if it is
red, we have seen the bug; if it is white, we have not. //p// is the number of
red balls in the box divided by the number of white balls. The total number of
balls in the box is not relevant to our analogy, only the red/white ratio. After
seeing the ball, we put it back in the box, as //p// must remain constant.




= Estimating p =

Before we try fixing anything, we have to know more about the bug, starting by
the probability //p// of reproducing it. We can estimate this probability by
dividing the number of times we see the bug //k// by the number of times we
tested for it //n//. Let's try that with our sample bug:

```
$ ./hasbug 
error
$ ./hasbug
25877
```

So: //k//=1, //n//=2 => //p// ~ 0.5, right? If we tested 1000 times and got
the bug exactly 500 times, what would be the difference?



== p precision ==

Let's try that with our box analogy: imagine that there are 3 balls in the box,
one red and two white. That means //p// = 1/3. What are the possible results
when we test twice?

|| First ball | Second ball | p estimate |
| red         | white       | 0.5        |
| white       | red         | 0.5        |
| white       | white       | 0          |
| red         | red         | 1          |

The less we test, the smaller is our precisions. Roughly, //p// precision will
be at most 1/n. That means that we have a precision of 0.001 when we test 1000
times.



== p probability ==

From another angle: if //p// is 1/3, what are the odds of seeing one
error in two tests? Let's name the 3 balls as A-red, B-white and C-white:

|| First ball | Second ball | p estimate |
| A-red       | A-red       | 1          |
| A-red       | B-white     | 0.5        |
| A-red       | C-white     | 0.5        |
| B-white     | A-red       | 0.5        |
| B-white     | B-white     | 0          |
| B-white     | C-white     | 0          |
| C-white     | A-red       | 0.5        |
| C-white     | B-white     | 0          |
| C-white     | C-white     | 0          |

We have 4/9 (~44%) probability of estimating //p// as 0.5 if it is 1/3 and we
test only twice. That's the same probability of estimating //p// as 0 in the
same conditions.



== Increasing //n// ==



From wikipedia, we can see that this can be calculated by
using the probability mass function of the binomial distribution:

 [http://upload.wikimedia.org/math/0/c/1/0c1ae7a35c20afa9f189dffa5d3c0c23.png] 

Where:

 [http://upload.wikimedia.org/math/c/2/d/c2d02458d8c35f11e465c639ba62f081.png] 

(we can use the function //dbinom// in R to calculate that)

By using those formulas, we can see that the probability of seeing one error in
two tests give that //p// is 0.2 is 0.32. That means that if we perform many
2-runs tests, we will see the bug once in 32% of the tests.

On the other hand, the probability of seeing 500 errors in 1000 runs for a //p//
of 0.2 is 3e-99. You would have to test more than once for
[each atom in the known universe http://en.wikipedia.org/wiki/Observable_universe#Matter_content]
on average to see that happen.

So, if we test twice, //p// might be 0.5, but there is a good chance that it
might be 0.2 also. 



