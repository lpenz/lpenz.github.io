Hard drive occupation prediction - part 3

2011-02-19
%!preproc: <mcsimulations> 10000


On the [second cwd$/../df0pred-2/index.html] article, we saw how to use a Monte
Carlo simulation generate sample of disk space delta for future dates and
calculate the distribution probability of zeroing free space in the future.

In this article, we will see how we can plot the predicted distribution for the
occupied disk space. Instead of answering que question "how likely is that my
disk space will zero before date X?," we will be able to also answer "how much
disk space will I need by date X, and with what probability?"



= The input data =

[This file $cwd$/duinfospike.dat] has the dataset we will use as example. It's
the same we used in the second part. The graph below shows it:

 [$cwd$/usd.png] 

We not import this data into R:

%!include: ``datain.R``

And we build our simulation for the next 4 months and create some variables:

%!include: ``mcscalc.R``



= Visualizing the possible scenarios =

After performing the simulation, what kind of data do we have? Each simulation is
built by sampling from the delta samples and adding to the current disk space
for each day in the simulated period. We can say that each simulation is a
possible scenario for the next 4 months. As the delta samples are concentrated
around a point, we can expect the same behaviour from the simulations: for each
day, they should be more concentrated around a range of values, even though the
range should grow wider with time.

The graph bellow shows the result of the first 5 simulations:

%!include: ``mcs3plot.R``

 [$cwd$/mcs3.png] 

From this graph we can clearly see that the range of possible values for the
used disk space grows with time. All simulations start with the same value - the
actual used disk space for today - and grow apart as we sample from the delta
pool.

We can also plot all simulations in a single graph:

%!include: ``mcsplot.R``

 [$cwd$/mcs.png] 

This plot gives us an idea of the overall spread of the data, but it fails to
show the density. Remember, there are <mcsimulations> black lines there.



= Visualizing the distribution for specific days =

There is another way to look at our data: we have created, for each day, a
sample of the possible used disk spaces. We can take any day of the simulation
and look at the density:

%!include: ``mcsdaydensplot.R``

 [$cwd$/mcsdaydens.png] 


By looking at this graph we can see the trend: the curves are getting
flatter - meaning that we are getting more possible values for occupied disk
space; and moving to the right - meaning that we have more simulations with
higher occupied disk space values.



= Visualizing the evolution of the distribution =

So far, we have seen how we can visualize some simulations along the 4 months
and how we can visualize the distribution for some specific days.

We can also plot the distribution of the values for each day in the simulated 4
months. We can't use the kernel density plot or the histogram, as they use both
axis, but there are other options.


== The Boxplot ==

We can use a boxplot for each day in R in a very straightforward way.

%!include:``mcsboxplot.R``

 [$cwd$/mcsbox.png] 

The boxplots glued together build a shape that shows us the distribution of our
simulations at any day:
- The thick line in the middle of the graph is the median
- The darker area goes from the first quartile to the third - which means that
  50% of the samples are in that range
- The lighter area has the maximum and minimum points, if they are within 1.5
  [IQR http://en.wikipedia.org/wiki/Interquartile_range] of the upper/lower
  quartile.  Points out of this range are considered outliers and are not
  plotted.


==============================================================================

= The evolution of the distribution =

To see how a sample is distributed we can use a
[histogram http://en.wikipedia.org/wiki/Histogram], a
[kernel density plot http://en.wikipedia.org/wiki/Kernel_density]
or a [boxplot http://en.wikipedia.org/wiki/Boxplot]. The histogram and the
kernel density plot use one axis for the data and the other for the count or
density. The boxplot, on the other hand, uses a single dimension for the data.
That serves us well, as we can have the date as one axis, the disk space as the
other and a boxplot for each day.

As an example, let's plot the cumulative distribution for the used disk space:

%!include: ``usdcumplot.R``

 [$cwd$/usdcum.png] 

What we have here is a lot of boxplots piled together, forming a shape that
tells us how the distribution of the used disk space evolved with each sample:
the thick line in the middle of the graph is the median; the darker area
goes from the first quartile to the third - which means that 50% of the samples
are in that range. The lighter area has the maximum and minimum points, if they
are within 1.5 [IQR http://en.wikipedia.org/wiki/Interquartile_range] of the
upper/lower quartile. Points out of that range are considered outliers and
plotted individually.

The graph above shows us the evolution of the distribution of the used disk
space. We have already seen that the difference in used disk space yields more
useful data:

%!include: ``deltacalc.R``
%!include: ``deltacumplot.R``

 [$cwd$/deltacum.png] 

We can see in the graph above that we have some small outliers to the top and
some big outliers in the bottom - those were disk cleanups, as you may recall.
By using the //outline=FALSE// option of boxplot, we can zoom in the middle of
the plot:

%!include: ``deltacumoutlineplot.R``

 [$cwd$/deltacumoutline.png] 

From this graph we can tell that our the perceived pattern of disk consumption
pattern is quite regular after we have processed half our samples.



= Distribution prediction =

The graph above shows us how the distribution of the actual occupied disk space
and its delta have evolved in the past. We can also see how our predicted
distribution will evolve in the future.

The first thing we need is the simulated data. Again, we use Monte Carlo, but
this time we keep all the samples from all the trials and boxplot them by date:

%!include: ``mcscalc.R``
%!include: ``mcsplot.R``

 [$cwd$/mcs.png] 


To find the set of number of days left from our <mcsimulations> samples, we do:

%!include: ``df0daysleftcalc.R``

We can then use the same procedure we used in the
[previous article cwd$/../df0pred-2/index.html]:

%!include: ``df0calc.R``
%!include: ``df0plot.R``

 [$cwd$/df0.png] 



----------------------------------------

% = The problem with the linear regression =
% 
% The linear regression used in the first article has a serious
% lack of [robustness http://en.wikipedia.org/wiki/Robust_statistics].
% That means that it is very sensitive to even single departures
% from the linear pattern. For instance, if we periodically delete some big
% files in the hard disk, we end up breaking the sample in parts that cannot be
% analysed together. If we plot the line given by the linear model, we can see
% clearly that it does not fit our overall data very well:
% 
%  [$cwd$/lm.png] 
% 
% ([Data file $cwd$/duinfospike.dat])
% 
% We can see in the graph that the linear model gives us a line that our free disk
% space is increasing instead of decreasing! If we use this model, we will reach
% the conclusion that we will never reach df0.
% 
% If we keep analysing used disk space, there is not much we can do besides
% discarding the data gathered before the last cleanup. There is no way to easily
% ignore only the cleanup.
% 
% In fact, we can only use the linear regression method when our disk consumption
% pattern is linear for the analysed period - and that rarely is the case
% when there is human intervention. We should always look at the graph to see if
% the model makes sense.
% 
% 
% 
% = A naïve new method: averaging the difference =
% 
% Instead of using the daily used disk space as input, we will use the
% daily **difference** (or delta) of used disk space. By itself, this reduces a
% big disk cleanup to a single outlier instead of breaking our sample. We could
% then just filter out the outliers, calculate the average daily increment in used
% disk space and divide the current free space by it. That would give us the
% average number of days left until disk exhaustion. Well, that would also give us
% some new problems to solve.
% 
% The first problem is that filtering out the outliers is neither
% straightforward nor recommended. Afterall, we are throwing out data that might
% be meaningful: it could be a regular monthly process that we should take into
% account to generate a better prediction.
% 
% Besides, by averaging disk consumption and dividing free disk space by it,  we
% would still not have the probability distribution for the date, only a single
% value.
% 
% 
% 
% = The real new method: days left by Monte Carlo simulation =
% 
% Instead of calculating the number of days left from the data, we will use a
% technique called [Monte Carlo simulation http://en.wikipedia.org/wiki/Monte_carlo_simulation]
% to generate the distribution of days left. The idea is simple: we sample the
% data we have - daily used disk space - until the sum is above the free disk
% space; the number of samples taken is the number of days left. By doing that
% repeatedly, we get the set of "possible days left" with a distribution that
% corresponds to the data we have collected. Let's how we can do that in R.
% 
% First, let's load the data file that we will use (same one used in the
% introduction) along with a variable that holds the size of the disk (500GB; all
% units are in MB):
% 
% %!include: ``datain.R``
% 
% We now get the delta of the disk usage. Let's take a look at it:
% 
% %!include: ``deltacalc.R``
% %!include: ``deltaplot.R``
% 
% 
%  [$cwd$/delta.png] 
% 
% 
% The summary function gives us the five-number summary, while the boxplot shows
% us how the data is distributed graphically:
% 
% %!include: ``deltasummary.txt``
% 
% %!include: ``deltaboxplot.R``
% 
% 
%  [$cwd$/deltabox.png] 
% 
% 
% The kernel density plot gives us about the same, but in another visual format:
% 
% %!include: ``deltakdplot.R``
% 
% 
%  [$cwd$/deltakd.png] 
% 
% 
% We can see the cleanups right there, as the lower points.
% 
% The next step is the creation of the sample of the number of days left until
% exhaustion. In order to do that, we create an R function that sums values taken
% randomly from our delta sample until our free space zeroes, and returns the
% number of samples taken:
% 
% %!include: ``func.R``
% 
% By repeatedly running this function and gathering the results, we generate a set
% of number-of-days-until-exhaustion that is robust and corresponds to the data we
% have observed. This robustness means that we don't even need to remove outliers,
% as they will not disproportionally bias out results:
% 
% %!include: ``daysleftcalc.R``
% %!include: ``daysleftplot.R``
% 
%  [$cwd$/daysleft.png] 
% 
% What we want now is the
% [empirical cumulative distribution http://en.wikipedia.org/wiki/Empirical_distribution_function].
% This function gives us the probability that we will reach df0 **before** the
% given date.
% 
% %!include: ``df0ecdfcalc.R``
% %!include: ``df0ecdfplot.R``
% 
%  [$cwd$/df0ecdf.png] 
% 
% 
% With the cumulative probability estimate, we can see when we have to start
% worrying about the disk by looking at the first day that the probability of df0
% is above 0:
% 
% %!include: ``df0first.txt``
% 
% Well, we can also be a bit more bold and wait until the chances of reaching df0
% rise above 5%:
% 
% %!include: ``df0above5.txt``
% 
% Mix and match and see what a good convention for your case is.
% 
% 
% 
% = Conclusion =
% 
% This and the [previous article $cwd$/../df0pred-1/index.html] showed how to use
% statistics in R to predict when free hard-disk space will zero.
% 
% The first article was main purpose was to serve as an introduction to R. There
% are many reasons that make linear regression an unsuitable technique for
% df0 prediction - the underlying process of disk consumption is certainly not
% linear. But, if the graph shows you that the line fits, there is no reason to
% ignore it.
% 
% Monte Carlo simulation, on the other hand, is a powerful and general technique.
% It assumes little about the data (non-parameterized), and it can give you
% probability distributions. If you want to forecast something, you can always
% start recording data and use Monte Carlo in some way to make predictions
% **based on the evidence**. Personally, I think we don't do this nearly as often
% as we could. Well, [Joel is even using it to make schedules http://www.joelonsoftware.com/items/2007/10/26.html].
% 
% 
% 
% = Further reading =
% 
% - http://www.joelonsoftware.com/items/2007/10/26.html: Joel's use of Monte Carlo
%   to make schedules.
% - http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29: Wikipedia's page
%   on bootstrapping, which is clearer than the one on Monte Carlo simulations.
% - http://www.r-bloggers.com/: daily news and tutorials about R, very good to
%   learn the language and see what people are doing with it.
% 


