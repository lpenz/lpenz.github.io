Data exploring with R: hard drive occupation prediction

2010-08-15



On some environments, disk space usage can be pretty predictable. In this post,
we will see how to do a linear regression to estimate when free space will reach
zero, and how to assess the quality of such regression, all using
[R http://en.wikipedia.org/wiki/R_programming_language] - the
statistical software environment.



= Prerequisites =

The first thing we need is the data. By running a simple
``(date --utc; df -k; echo) >> /var/dflog.txt``
everyday at 00:00 by cron, we will have more than enough, as that will store the
date along with total, free and used space for all mounted devices.

On the other hand, that is not really easy to parse in R, unless we learn more
about the language. In order to keep this post short, we invite the reader to
use his favorite scripting language (or python) to process that into a file with
the day in the first column and the occupied space in the second, and a row for
each day:
```
YYYY-MM-DD free space
YYYY-MM-DD free space
(...)
```

This format can be read and parsed in R with a single command.

Through the article, we will be using [this data file $cwd$/duinfo.dat] as example.
The file was generated by observing real data, making the regression and adding
back normal error to the regression. Even though it is not real, it is based
upon values observed in real data. The used disk space is represented in
1K-blocks.



= Starting up =

After running **R** in the shell prompt, we get the usual license and basic help
information.

The first step is to import your data:
```
> duinfo <- read.table('duinfo.dat', colClasses=c("Date","numeric"), col.names=c("day","usd"))
> attach(duinfo)
```

The variable //duinfo// is now a list with two columns: //day// and //usd//. The
``attach`` command allows us to use the column names directly.

We can check the data graphically by issuing:
```
> plot(usd ~ day)
```
That gives us an idea on how predictable the usage of our hard drive is.

From our example, we get:
 [$cwd$/pointplot.png] 



= Linear model =

We can now create and take a look at our linear model object:
```
> model <- lm(usd ~ day)
> model
```
%!include: ``lm.txt``

The second coefficient in the example tells us that we are consuming about 458 MB (4.580e+05) of disk space per day.

We can also plot the line over our data:
```
> abline(model)
```

The example plot, with the line:
 [$cwd$/lmplot.png] 



= Evaluating the model =

R provides us with a very generic command that generates statistical information
about objects: **summary**. Let's use it on our linear model objects:
```
> summary(model)
```
%!include: ``lmsummary.txt``

To check the quality of a linear regression, we focus on the **residuals**, as
they represent the error of our model. We calculate them by subtracting the
expected value (from the model) from the sampled value, for every sample. The
first information shown in the summary above is the
[five-number summary http://en.wikipedia.org/wiki/Five-number_summary]
of the residuals. That tells us the maximum and minimum error, and that 75% of
the errors are between -1.2 GB and 1.8 GB. We then get the results of a
[Student's t-test http://en.wikipedia.org/wiki/Student%27s_t-test] of
the model coefficients against the data. The last column tells us roughly how
probable seeing the given residuals is, assuming that the disk space does not
depend on the date - it's the
[p-value http://en.wikipedia.org/wiki/P-value]. We usually accept an
hypothesis when the p-value is less than 5%; in this example, we have a large
margin for both coefficients. The last three lines of the summary give us more
measures of fit: the
[r-squared http://en.wikipedia.org/wiki/R-squared] values - the closest
to 1, the better; and the general p-value from the f-statistics, less than 5%
again.

In order to show how bad a linear model can be, the summary bellow was generated
by using 50GB as the disk space and adding a random value between -1GB and 1GB
each day:

%!include: ``lmrandsummary.txt``

It's easy to notice that, even though the five-number summary is narrower, the
p-values are greater than 5%, and the r-squared values are very far from 1.

Now that we are convinced (hopefully) that our linear model fits our data
nicely, we can use it to predict hard-disk shortage.



= Predicting disk-free-zero =

Until now, we represented disk space as a function of time, creating a model
that allows us to predict the used disk space given the date. But what we really
want now is to predict the date our disk will be full. In order to do that, we
have to invert the model. Fortunately, all statistical properties (t-tests,
f-statistics) hold in the inverted model.

```
> model2 <- lm(day ~ usd)
```

We now use the **predict** function to extrapolate the model. Let's assume that
the total disk space in our example is 1TB:

```
> predict(model2, data.frame(usd = 1e9))
       1
16280.59
```

But... when is that? Well, that is the numeric representation of a day in R,
which is the number of days since 1970-01-01. To get the human-readable day, we
use:

```
> as.Date(predict(model2, data.frame(usd = 1e9)), origin="1970-01-01")
           1
"2014-07-29"
```

There we are: our disk space will reach 1TB in 2014-07-29 **if** the current
pattern holds until then, with 95% statistical significance.



= Conclusion =

By analysing the used disk space, we have avoided dealing with total disk
expansion. We can even predict how long the expansion will give us before
another one is required.

On the other hand, looking only at the used space has a fatal weakness: we
cannot deal with spikes. A spike could originate from an upgrade or from the
installation of some software, or even from a cleanup of the hard drive. In our
data, the spike would break our sample in two groups, as all samples after it
will have an offset. The model would give us a line that tries to fit both
groups, acceptably fitting none. To fix the situation we can either throw out
all data prior to the spike or decrease the spike delta from the total hard
drive space and from the following samples before calculating the model. Both
solutions are inadequate. A better approach is to avoid analysing used disk
space and look only at its daily difference. That would make the spike appear as
a single sample that has a reduced impact in our prediction, and we can easily
eliminate it as an outlier without further problems. But that is the subject for
another article.



= Further reading =

- http://www.cyclismo.org/tutorial/R/index.html: R tutorial
- http://www.r-tutor.com/: An R introduction to statistics
- http://cran.r-project.org/doc/contrib/Lemon-kickstart/index.html: Kickstarting R



