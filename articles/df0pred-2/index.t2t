Hard drive occupation prediction - part 2

?


On the [first $cwd$/../df0pred-1/index.html] article, we saw a quick-and-dirty method to
predict disk space exhaustion when the usage pattern is rigorously linear. We did that by
importing our data into [R http://en.wikipedia.org/wiki/R_programming_language]
and making a linear regression.

In this article we will see the problems with that method, and deploy a
more robust solution. Besides robustness, we will also see how we can generate a
probability distribution for the date of disk space exhaustion instead of
calculating a single day.



= The problem with the linear regression =

The linear regression used in the first article has a serious
lack of [robustness http://en.wikipedia.org/wiki/Robust_statistics].
That means that it is very sensitive to even single departures
from the linear pattern. For instance, if we ever delete some big files in the
hard disk, we would break the sample in two parts: one before the cleanup and
another, with an offset, after the cleanup. If we plot the line given by the
linear model, we can see clearly that it does not fit our overall data very
well:

 [lm.png] 

([Data file $cwd$/duinfospike.dat])

If we keep analysing the occupied disk space, there is not much we can do
besides discarding the data we got before the cleanup, even though it was
perfectly valid. There is no way to easily ignore the cleanup and use the data.

In fact, we can only use the first method when our disk consumption pattern is
linear for the whole analysed period - and that rarely is the case when there is
human intervention. We should always look at the graph to see if the solution
makes sense.



= A naïve new method: averaging the difference =

Instead of using the daily used disk space as input, we will use the
daily **difference** (or delta) of used disk space. By itself, this reduces a
big disk cleanup to a single outlier, instead of breaking our sample in two. We
could then just filter out the outliers, calculate the average daily increment in used
disk space and divide the current free space by it. That would give us the
average number of days left until disk exhaustion. Well, that would give us some
new problems to solve.

The first problem is that filtering out the outliers is not
simple. Afterall, we are throwing out data that might be valuable: it could be a
regular monthly process that we should take into account in order to generate
a better prediction.

Besides, we would still not have the probability distribution for the date, only
a single value.



= The real new method: bootstrapping days left =

Instead of calculating the number of days left from the data, we will use a
technique called [bootstrapping http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29]
to generate the distribution of days left. The idea is simple: we sample the
data we have - daily used disk space - until the sum is above the free disk
space; the number of samples taken is the number of days left. By doing that
repeatedly, we get the set of "possible days left" with a distribution that
corresponds to the data we have used. Let's how we can do that in R.

First, let's load the data file that we will use (same one used in the
introduction) along with a variable that holds the size of the disk (1TB; all
units are in MB):

%!include: ``datain.R``

We now get the delta of the disk usage, and take a look at its boxplot:

%!include: ``deltaboxcalc.R``
%!include: ``deltaboxplot.R``

 [deltabox.png] 

We can see the cleanup right there, as the lowest point.

The next step is the creation of the sample of the number of days left until
exhaustion. In order to do that, we create an R function that sums values taken
randomly from our disk delta sample until our free space zeroes, and returns the
number of samples taken:

%!include: ``func.R``

By repeatedly running this function and gathering the results, we generate a set
of number-of-days-until-exhaustion that is robust and corresponds to the data we
have observed. This robustness means that we don't even need to remove outliers,
as they will barely affect out results:

%!include: ``daysleftcalc.R``
%!include: ``daysleftplot.R``

 [daysleft.png] 


Let's use a trick to see the dates in that graph:

%!include: ``df0densityplot.R``

 [df0density.png] 


That represents the probability that we will reach df0 in any one of those
single days. More useful is the cumulative distribution, which is the
probability that we reach df0 **before** the given day:

%!include: ``df0cumsumplot.R``

 [df0cumsum.png] 


