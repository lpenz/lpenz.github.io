Hard drive occupation prediction - part 2

?


On the [first $cwd$/../df0pred-1/index.html] article, we saw a quick-and-dirty method to
predict disk space exhaustion when the usage pattern is rigorously linear. We did that by
importing our data into [R http://en.wikipedia.org/wiki/R_programming_language]
and making a linear regression.

In this article we will see the problems with that method, and deploy a
more robust solution. Besides robustness, we will also see how we can generate a
probability distribution for the date of disk space exhaustion instead of
calculating a single day.



= The problem with the linear regression =

The linear regression used in the first article has a serious
lack of [robustness http://en.wikipedia.org/wiki/Robust_statistics].
That means that it is very sensitive to even single departures
from the linear pattern. For instance, if we ever delete some big files in the
hard disk, we would break the sample in two parts: one before the cleanup and
another, with an offset, after the cleanup. If we plot the line given by the
linear model, we can see clearly that it does not fit our overall data very
well:

 [lmplotspike.png] 

([Data file $cwd$/duinfospike.dat])

If we keep analysing the occupied disk space, there is not much we can do
besides discarding the data we got before the cleanup, even though it was
perfectly valid. There is no way to easily ignore the cleanup and use the data.

In fact, we can only use the first method when our disk consumption pattern is
linear for the whole analysed period - and that rarely is the case when there is
human intervention. We should always look at the graph to see if the solution
makes sense.



= A naïve new method: averaging the difference =

Instead of using the daily used disk space as input, we will use the
daily **difference** (or delta) of used disk space. By itself, this reduces a
big disk cleanup to a single outlier, instead of breaking our sample in two. We
could then just filter out the outliers, calculate the average daily increment in used
disk space and divide the current free space by it. That would give us the
average number of days left until disk exhaustion. Well, that would give us some
new problems to solve.

The first problem is that filtering out the outliers is not
simple. Afterall, we are throwing out data that might be valuable: it could be a
regular monthly process that we should take into account in order to generate
a better prediction.

Besides, we would still not have the probability distribution for the date, only
a single value.



= The real new method: bootstrapping days left =

Instead of calculating the number of days left from the data, we will use a
technique called [bootstrapping http://en.wikipedia.org/wiki/Bootstrapping_(statistics)]
to generate the distribution of days left. The idea is simple: we sample the
data we have - daily used disk space - until the sum is above the free disk
space; the number of samples taken is the number of days left. By doing that
repeatedly, we get the set of "possible days left" with a distribution that
corresponds to the data we have used. Let's how we can do that in R.

First, let's load the data file that we will use (same one used in the
introduction):

```
> duinfo <- read.table('duinfospike.dat', colClasses=c("Date","numeric"), col.names=c("day","usd"))
> attach(duinfo)
```

We now get the delta of the disk usage, and take a look at its boxplot:

```
> dudelta <- diff(usd)
> boxplot(dudelta)
```

 [boxplot.png] 

We can see the cleanup right there, as the lowest point.

The next step is the creation of the sample of the number of days left until
exhaustion. In order to do that, we create an R function that sums values taken
randomly from our disk delta sample until our free space zeroes, and returns the
number of samples taken:

```
f <- function(spaceleft) {
    days <- 0
    while(spaceleft > 0) {
        days <- days + 1
        spaceleft <- spaceleft - sample(dudelta, 1, replace=TRUE)
    }
    days
}

```

By repeatedly running this function and gathering the results, we generate a set
of number-of-days-until-exhaustion that is robust and corresponds to the data we
have observed. This robustness means that we don't even need to remove outliers,
as they will barely affect out results. For an HD of 1TB (all units are 1kB):

```
freespace <- 1e9 - duinfo$usd[length(duinfo$usd)]
daysleft <- replicate(5000, f(freespace, 0))
plot(density(daysleft), bw=0.5)
```

 [daysleft.png] 



--------------------


On some environments, disk space usage can be pretty predictable. In this post,
we will see how to do a linear regression to estimate when free space will reach
zero, and how to assess the quality of such regression, all using
[R http://en.wikipedia.org/wiki/R_programming_language] - the
statistical software environment.



= Prerequisites =

The first thing we need is the data. By running a simple
``(date --utc; df -k; echo) >> /var/dflog.txt``
everyday at 00:00 by cron, we will have more than enough, as that will store the
date along with total, free and used space for all mounted devices.

On the other hand, that is not really easy to parse in R, unless we learn more
about the language. In order to keep this post short, we invite the reader to
use his favorite scripting language (or python) to process that into a file with
the day in the first column and the occupied space in the second, and a row for
each day:
```
YYYY-MM-DD free space
YYYY-MM-DD free space
(...)
```

This format can be read and parsed in R with a single command.

Through the article, we will be using [this data file $cwd$/duinfo.dat] as example.
The file was generated by observing real data, making the regression and adding
back normal error to the regression. Even though it is not real, it is based
upon values observed in real data. The used disk space is represented in
1K-blocks.



= Starting up =

After running **R** in the shell prompt, we get the usual license and basic help
information.

The first step is to import your data:
```
> duinfo <- read.table('duinfo.dat', colClasses=c("Date","numeric"), col.names=c("day","usd"))
> attach(duinfo)
```

The variable //duinfo// is now a list with two columns: //day// and //usd//. The
``attach`` command allows us to use the column names directly.

We can check the data graphically by issuing:
```
> plot(usd ~ day)
```
That gives us an idea on how predictable the usage of our hard drive is.

From our example, we get:
 [$cwd$/pointplot.png] 



= Linear model =

We can now create and take a look at our linear model object:
```
> model <- lm(usd ~ day)
> model
```
%%!include: ``lm.txt``

The second coefficient in the example tells us that we are consuming about 458 MB (4.580e+05) of disk space per day.

We can also plot the line over our data:
```
> abline(model)
```

The example plot, with the line:
 [$cwd$/lmplot.png] 



= Evaluating the model =

R provides us with a very generic command that generates statistical information
about objects: **summary**. Let's use it on our linear model objects:
```
> summary(model)
```
%%!include: ``lmsummary.txt``

To check the quality of a linear regression, we focus on the **residuals**, as
they represent the error of our model. We calculate them by subtracting the
expected value (from the model) from the sampled value, for every sample. The
first information shown in the summary above is the
[five-number summary http://en.wikipedia.org/wiki/Five-number_summary]
of the residuals. That tells us the maximum and minimum error, and that 75% of
the errors are between -1.2 GB and 1.8 GB. We then get the results of a
[Student's t-test http://en.wikipedia.org/wiki/Student%27s_t-test] of
the model coefficients against the data. The last column tells us roughly how
probable seeing the given residuals is, assuming that the disk space does not
depend on the date - it's the
[p-value http://en.wikipedia.org/wiki/P-value]. We usually accept an
hypothesis when the p-value is less than 5%; in this example, we have a large
margin for both coefficients. The last three lines of the summary give us more
measures of fit: the
[r-squared http://en.wikipedia.org/wiki/R-squared] values - the closest
to 1, the better; and the general p-value from the f-statistics, less than 5%
again.

In order to show how bad a linear model can be, the summary bellow was generated
by using 50GB as the disk space and adding a random value between -1GB and 1GB
each day:

%%!include: ``lmrandsummary.txt``

It's easy to notice that, even though the five-number summary is narrower, the
p-values are greater than 5%, and the r-squared values are very far from 1.

Now that we are convinced (hopefully) that our linear model fits our data
nicely, we can use it to predict hard-disk shortage.



= Predicting disk-free-zero =

Until now, we represented disk space as a function of time, creating a model
that allows us to predict the used disk space given the date. But what we really
want now is to predict the date our disk will be full. In order to do that, we
have to invert the model. Fortunately, all statistical properties (t-tests,
f-statistics) hold in the inverted model.

```
> model2 <- lm(day ~ usd)
```

We now use the **predict** function to extrapolate the model. Let's assume that
the total disk space in our example is 1TB:

```
> predict(model2, data.frame(usd = 1e9))
       1
16280.59
```

But... when is that? Well, that is the numeric representation of a day in R,
which is the number of days since 1970-01-01. To get the human-readable day, we
use:

```
> as.Date(predict(model2, data.frame(usd = 1e9)), origin="1970-01-01")
           1
"2014-07-29"
```

There we are: our disk space will reach 1TB in 2014-07-29 **if** the current
pattern holds until then, with 95% statistical significance.



= Conclusion =

By analysing the used disk space, we have avoided dealing with total disk
expansion. We can even predict how long the expansion will give us before
another one is required.

On the other hand, looking only at the used space has a fatal weakness: we
cannot deal with spikes. A spike could originate from an upgrade or from the
installation of some software, or even from a cleanup of the hard drive. In our
data, the spike would break our sample in two groups, as all samples after it
will have an offset. The model would give us a line that tries to fit both
groups, acceptably fitting none. To fix the situation we can either throw out
all data prior to the spike or decrease the spike delta from the total hard
drive space and from the following samples before calculating the model. Both
solutions are inadequate. A better approach is to avoid analysing used disk
space and look only at its daily difference. That would make the spike appear as
a single sample that has a reduced impact in our prediction, and we can easily
eliminate it as an outlier without further problems. But that is the subject for
another article.



= Further reading =

- http://www.cyclismo.org/tutorial/R/index.html: R tutorial
- http://www.r-tutor.com/: An R introduction to statistics
- http://cran.r-project.org/doc/contrib/Lemon-kickstart/index.html: Kickstarting R



