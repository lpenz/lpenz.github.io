Hard drive occupation prediction - part 2

?


On the [first $cwd$/../df0pred-1] article, we saw a quick-and-dirty method to
predict disk space exhaustion when the usage pattern is constant. We did that by
importing our data into [R http://en.wikipedia.org/wiki/R_programming_language]
and making a linear regression.

In this article we will see the exact problem with that method, and deploy a
more robust one. Besides robustness, we will also see how we can generate a
probability distribution for the date of disk space exhaustion instead of
calculating a single value.



= The problem with the linear regression =

The linear regression used in the first article has a fatal problem: it fails completely
when there is even a single day where disk usage is out-of-pattern in the
sample. Let's say that you deleted some big backup files in the hard disk, as
they were not needed anymore - that would break the sample in two parts: one
before the cleanup and another, with an offset, after the cleanup. If we plot
the line given by the linear regression, we can see clearly that our model fits
none of the parts:

 [lmplotsplit.png] 

If we keep analysing the occupied disk space, there is not much we can do
besides discarding the data we got before the cleanup, even though it was
perfectly valid. There is no way to discard **just** the cleanup and keep the
data. Or is there?

By dropping the linear regression and the occupied disk space analysis, and
looking at the **difference** in occupied space, we can simply ignore the
cleanup sample, as it does not affect any other sample. We just have to develop
a method to discover when the free space will zero based on the new data.



= The new method: sampling days left with bootstrapping =

If we just take the average of the occupied disk space delta and divide our
current free disk space by it, we get the number of days left before exhaustion,
on average. That would be equivalent to our linear regression, but,
unfortunately, is not a very realistic result, as it doesn't take into account
the variance of the data - there is a good chance that our disk space will zero
long before that if that variance is large and we are not lucky.

Instead of employing simple math, we can use our new friend R to generate a
probability distribution of the day of exhaustion. Let's see how we can do that.

First, let's load the data file that we used to generate the figure above
([download $cwd$/duinfospike.dat]):

```
> duinfo <- read.table('duinfospike.dat', colClasses=c("Date","numeric"), col.names=c("day","usd"))
> attach(duinfo)
```

We now get the delta of the disk usage, and take a look at its boxplot:

```
> dudelta <- diff(usd)
> boxplot(dudelta)
```

 [boxplot.png] 

We can see the cleanup right there, as the lower point.

The next step is the creation of the sample of the number of days left until
exhaustion. In order to do that, we create an R function that sums values taken
randomly from our disk delta sample until our free space zeroes, and returns the
number of samples taken:

```
f <- function(spaceleft, days)
    if (spaceleft <= 0) days else f(spaceleft - sample(duinfo$usd, 1, replace=TRUE), days + 1)
```

By repeatedly running this function and gathering the results, we generate a set
of number-of-days-until-exhaustion that is robust and corresponds to the data we
have observed. This robustness means that we don't even need to remove outliers,
as they will barely affect out results.



--------------------


On some environments, disk space usage can be pretty predictable. In this post,
we will see how to do a linear regression to estimate when free space will reach
zero, and how to assess the quality of such regression, all using
[R http://en.wikipedia.org/wiki/R_programming_language] - the
statistical software environment.



= Prerequisites =

The first thing we need is the data. By running a simple
``(date --utc; df -k; echo) >> /var/dflog.txt``
everyday at 00:00 by cron, we will have more than enough, as that will store the
date along with total, free and used space for all mounted devices.

On the other hand, that is not really easy to parse in R, unless we learn more
about the language. In order to keep this post short, we invite the reader to
use his favorite scripting language (or python) to process that into a file with
the day in the first column and the occupied space in the second, and a row for
each day:
```
YYYY-MM-DD free space
YYYY-MM-DD free space
(...)
```

This format can be read and parsed in R with a single command.

Through the article, we will be using [this data file $cwd$/duinfo.dat] as example.
The file was generated by observing real data, making the regression and adding
back normal error to the regression. Even though it is not real, it is based
upon values observed in real data. The used disk space is represented in
1K-blocks.



= Starting up =

After running **R** in the shell prompt, we get the usual license and basic help
information.

The first step is to import your data:
```
> duinfo <- read.table('duinfo.dat', colClasses=c("Date","numeric"), col.names=c("day","usd"))
> attach(duinfo)
```

The variable //duinfo// is now a list with two columns: //day// and //usd//. The
``attach`` command allows us to use the column names directly.

We can check the data graphically by issuing:
```
> plot(usd ~ day)
```
That gives us an idea on how predictable the usage of our hard drive is.

From our example, we get:
 [$cwd$/pointplot.png] 



= Linear model =

We can now create and take a look at our linear model object:
```
> model <- lm(usd ~ day)
> model
```
%%!include: ``lm.txt``

The second coefficient in the example tells us that we are consuming about 458 MB (4.580e+05) of disk space per day.

We can also plot the line over our data:
```
> abline(model)
```

The example plot, with the line:
 [$cwd$/lmplot.png] 



= Evaluating the model =

R provides us with a very generic command that generates statistical information
about objects: **summary**. Let's use it on our linear model objects:
```
> summary(model)
```
%%!include: ``lmsummary.txt``

To check the quality of a linear regression, we focus on the **residuals**, as
they represent the error of our model. We calculate them by subtracting the
expected value (from the model) from the sampled value, for every sample. The
first information shown in the summary above is the
[five-number summary http://en.wikipedia.org/wiki/Five-number_summary]
of the residuals. That tells us the maximum and minimum error, and that 75% of
the errors are between -1.2 GB and 1.8 GB. We then get the results of a
[Student's t-test http://en.wikipedia.org/wiki/Student%27s_t-test] of
the model coefficients against the data. The last column tells us roughly how
probable seeing the given residuals is, assuming that the disk space does not
depend on the date - it's the
[p-value http://en.wikipedia.org/wiki/P-value]. We usually accept an
hypothesis when the p-value is less than 5%; in this example, we have a large
margin for both coefficients. The last three lines of the summary give us more
measures of fit: the
[r-squared http://en.wikipedia.org/wiki/R-squared] values - the closest
to 1, the better; and the general p-value from the f-statistics, less than 5%
again.

In order to show how bad a linear model can be, the summary bellow was generated
by using 50GB as the disk space and adding a random value between -1GB and 1GB
each day:

%%!include: ``lmrandsummary.txt``

It's easy to notice that, even though the five-number summary is narrower, the
p-values are greater than 5%, and the r-squared values are very far from 1.

Now that we are convinced (hopefully) that our linear model fits our data
nicely, we can use it to predict hard-disk shortage.



= Predicting disk-free-zero =

Until now, we represented disk space as a function of time, creating a model
that allows us to predict the used disk space given the date. But what we really
want now is to predict the date our disk will be full. In order to do that, we
have to invert the model. Fortunately, all statistical properties (t-tests,
f-statistics) hold in the inverted model.

```
> model2 <- lm(day ~ usd)
```

We now use the **predict** function to extrapolate the model. Let's assume that
the total disk space in our example is 1TB:

```
> predict(model2, data.frame(usd = 1e9))
       1
16280.59
```

But... when is that? Well, that is the numeric representation of a day in R,
which is the number of days since 1970-01-01. To get the human-readable day, we
use:

```
> as.Date(predict(model2, data.frame(usd = 1e9)), origin="1970-01-01")
           1
"2014-07-29"
```

There we are: our disk space will reach 1TB in 2014-07-29 **if** the current
pattern holds until then, with 95% statistical significance.



= Conclusion =

By analysing the used disk space, we have avoided dealing with total disk
expansion. We can even predict how long the expansion will give us before
another one is required.

On the other hand, looking only at the used space has a fatal weakness: we
cannot deal with spikes. A spike could originate from an upgrade or from the
installation of some software, or even from a cleanup of the hard drive. In our
data, the spike would break our sample in two groups, as all samples after it
will have an offset. The model would give us a line that tries to fit both
groups, acceptably fitting none. To fix the situation we can either throw out
all data prior to the spike or decrease the spike delta from the total hard
drive space and from the following samples before calculating the model. Both
solutions are inadequate. A better approach is to avoid analysing used disk
space and look only at its daily difference. That would make the spike appear as
a single sample that has a reduced impact in our prediction, and we can easily
eliminate it as an outlier without further problems. But that is the subject for
another article.



= Further reading =

- http://www.cyclismo.org/tutorial/R/index.html: R tutorial
- http://www.r-tutor.com/: An R introduction to statistics
- http://cran.r-project.org/doc/contrib/Lemon-kickstart/index.html: Kickstarting R



