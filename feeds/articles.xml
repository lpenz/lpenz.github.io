<?xml version="1.0"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Avulsos by Penz - Articles</title>
        <link>http://www.lpenz.org</link>
        <description>Articles in Avulsos by Penz page.</description>
        <managingEditor>lpenz@lpenz.org (Leandro Lisboa Penz)</managingEditor>
        <webMaster>lpenz@lpenz.org (Leandro Lisboa Penz)</webMaster>
        <docs>http://www.rssboard.org/rss-specification</docs>

        <pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate>
        <lastBuildDate>Wed, 05 Apr 2023 00:00:00 +0000</lastBuildDate>

        <language>en</language>
        <image>
            <title>Avulsos by Penz - Articles</title>
            <link>http://www.lpenz.org</link>
            <url>http://www.lpenz.org/logo-black.png</url>
        </image>
        <atom:link href="http://www.lpenz.org/feeds/articles.xml" rel="self" type="application/rss+xml"/>


	
		<item>
			<title>Provisioning a Raspberry Pi using ansible</title>
			<link>http://www.lpenz.org/articles/ansiblerpi</link>
			<guid>http://www.lpenz.org/articles/ansiblerpi</guid>
			<pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
These are my notes on how to get started provisioning Raspbian with
ansible.
</p>

<section>
<h1>Before using ansible</h1>

<p>
We have to get a working Raspbian installation before using ansible on
the device.
</p>

<section>
<h2>Installing Raspbian</h2>

<p>
The first step is downloading and installing Raspbian in the SD card
you're going to use in the Pi. You can find the Raspbian image here:
<a href="https://www.raspberrypi.org/downloads/raspbian/">https://www.raspberrypi.org/downloads/raspbian/</a>. Prefer the "lite"
image, as it allows you to start from a cleaner point.
</p>
<p>
To install the image, use an adapter to plug the SD card into a
computer. You can figure out the <code>/dev/</code> device that got assigned by
using <code>lsblk</code> before and after plugging the card. Let's assume it
got assigned to <code>/dev/sdc</code>, for the sake of illustration. The next
step is writting the Raspbian image to it:
</p>

<pre>
unzip -p *-raspbian-stretch-lite.zip | sudo dd of=/dev/sdc bs=4M oflag=dsync status=progress
</pre>

<p>
You can now insert the card into the Raspberry Pi board.
</p>

</section>
<section>
<h2>Initial Raspbian setup</h2>

<p>
Raspbian comes with a initial user <strong>pi</strong>, password <strong>raspberry</strong>,
that can use sudo freely.  <strong>ssh</strong> is installed, but
disabled. Changing the password and enabling ssh are the next steps,
in a console session that should look like the following:
</p>

<pre>
&gt; raspberrypi login: pi
&gt; Password:
passwd
&gt; Changing password for pi.
&gt; (current) UNIX password:
&gt; Enter new UNIX password:
&gt; Retype new UNIX password:
&gt; passwd: password updated successfully
sudo systemctl enable ssh.service
(...)
</pre>

</section>
<section>
<h2>Alternative initial setup</h2>

<p>
Another way to do the initial image setup is by mounting in your system and
changing the required files before writting it in the SD card. To mount the
image:
</p>

<ul>
<li>Extract it from the zip file:
<p></p>

<pre>
unzip *-raspbian-*-lite.zip
</pre>

<p></p>
</li>
<li>Using <code>losetup</code>, map the file to a loopback device, and list the loopback
  devices to figure out where it was mapped:
<p></p>

<pre>
sudo losetup -f *-raspbian-*-lite.img
sudo losetup -l
&gt; NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE             DIO LOG-SEC
&gt; /dev/loop0         0      0         0  0 *-raspbian-*-lite.img   0     512
</pre>

<p></p>
From here on we assume that the file was mapped to <code>/dev/loop0</code>.
<p></p>
</li>
<li>Tell the kernel to make the partitions available at <code>/dev/</code> using
  <code>partx</code>, figure out where they are using <code>lsblk</code>:
<p></p>

<pre>
sudo partx -a /dev/loop0
lsblk /dev/loop0
&gt; NAME      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
&gt; loop0       7:0    0  1.8G  0 loop
&gt; ├─loop0p1 259:0    0 43.8M  0 loop
&gt; └─loop0p2 259:1    0  1.7G  0 loop
</pre>

<p></p>
</li>
<li>Mount the partition in a temporary directory:
<p></p>

<pre>
mkdir rpi
sudo mount /dev/loop0p2 rpi
</pre>

<p></p>
</li>
<li>You can now modify the files in the <code>rpi</code> directory. Use
  <code>openssl passwd -1 &lt;password&gt;</code> to get the string to use in the
  <code>etc/shadow</code> file as the password of the <code>pi</code> user. You can also change
  also change the name of the user, just be careful to change it everywhere
  needed:
  <ul>
  <li><code>/etc/passwd</code>
  </li>
  <li><code>/etc/shadow</code>
  </li>
  <li><code>/etc/subuid</code>
  </li>
  <li><code>/etc/group</code>
  </li>
  <li><code>/etc/gshadow</code>
  </li>
  <li><code>/etc/subgid</code>
  </li>
  <li>And rename <code>/home/pi</code>
  </li>
  </ul>
<p></p>
  To enable ssh, you should create init and systemd links:
<p></p>

<pre>
sudo ln -s /lib/systemd/system/ssh.service rpi/etc/systemd/system/sshd.service
sudo ln -s /lib/systemd/system/ssh.service rpi/etc/systemd/system/multi-user.target.wants/ssh.service
for d in rc2.d rc3.d rc4.d rc5.d; do (cd rpi/etc/$d; sudo ln -sf ../init.d/ssh S01ssh); done
find rpi/etc/rc* -name 'K*ssh' -exec sudo rm {} +
</pre>

<p></p>
  Other, security related things to do:
<p></p>

<pre>
sudo rm rpi/etc/sudoers.d/010_pi-nopasswd
</pre>

<p></p>
  You can also change other things if you want. When you are done, proceed to
  the next item.
<p></p>
</li>
<li>Unmount the partition, remove the temporary directory and unmap the loopback
  device:
<p></p>

<pre>
sudo umount rpi
rmdir rpi
sudo losetup -d /dev/loop0
</pre>

<p></p>
</li>
<li>Write the image to the SD card as before:
<p></p>

<pre>
sudo dd if=*-raspbian-*-lite.img of=/dev/sdc bs=4M oflag=dsync status=progress
</pre>

</li>
</ul>

</section>
</section>
<section>
<h1>Using ansible</h1>

<p>
You can now use ansible from any computer to provision the Pi. I'd
recommend, though, using <em>python3</em> as the interpreter on the Pi, as
it has more modules available - *apt_repository*, for instance, is not
directly usable with the default python2 interpreter.
</p>
<p>
A basic playbook that runs tasks as root looks like the following
(save it as <code>playbook-sudo-rpi.yml</code>):
</p>

<pre>
---
- hosts: all
  become: yes
  become_user: root
  vars:
    ansible_python_interpreter: /usr/bin/python3
  tasks:
    - debug: msg="Ansible running in {{ansible_lsb.id}}!"
</pre>

<p>
Assuming that the playbook as available as <code>playbook.yml</code>, and that
<code>raspberrypi</code> is a name that can be resolved to the device, you can
run the playbook with the following command:
</p>

<pre>
ansible-playbook -i raspberrypi, -u pi playbook-sudo-rpi.yml -K
</pre>

<p>
(you can also replace <code>raspberrypi</code> with the actual IP address of
the device)
</p>

<section>
<h2>Setting up Debian repositories</h2>

<p>
Because Raspbian is based on Debian and upstream Debian has support for the
architecture of the Raspberry Pi B+, you can configure and use upstream Debian
repositories and packages in the Pi. To do that, use the follwing snippet:
</p>

<pre>
(...)
  tasks:
    - name: install apt pinning config
      copy: src=apt-pinning dest=/etc/apt/preferences.d/99pinning owner=root group=root mode=0644
    - name: install Debian apt key
      apt_key: url='https://ftp-master.debian.org/keys/archive-key-9.asc' id='E1CF20DDFFE4B89E802658F1E0B11894F66AEC98' state=present
      notify: apt-update
    - name: install Debian apt repositories
      apt_repository: repo='deb http://ftp.debian.org/debian {{item}} main contrib non-free' state=present
      with_items:
        - stable
        - testing
        - unstable
        - experimental
      notify: apt-update
  handlers:
    - name: apt-update
      apt: update-cache=yes
</pre>

<p>
The <a href="apt-pinning">apt-pinning</a> file should contain something like the following:
</p>

<pre>
Package: *
Pin: release o=Raspberry Pi Foundation
Pin-Priority: 600

Package: *
Pin: release o=Raspbian
Pin-Priority: 600

Package: *
Pin: release a=stable
Pin-Priority: 510

Package: *
Pin: release a=testing
Pin-Priority: 520

Package: *
Pin: release a=unstable
Pin-Priority: 150

Package: *
Pin: release a=experimental
Pin-Priority: 120
</pre>

</section>
</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Probabilistic bug hunting</title>
			<link>http://www.lpenz.org/articles/bugprobhunt</link>
			<guid>http://www.lpenz.org/articles/bugprobhunt</guid>
			<pubDate>Mon, 09 Dec 2013 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
Have you ever run into a bug that, no matter how careful you are trying to
reproduce it, it only happens sometimes? And then, you think you've got it, and
finally solved it - and tested a couple of times without any manifestation. How
do you know that you have tested enough? Are you sure you were not "lucky" in
your tests?
</p>
<p>
In this article we will see how to answer those questions and the math
behind it without going into too much detail. This is a pragmatic guide.
</p>

<section~A~>
<h1></h1>
<section id="thebug">
<h2>The Bug</h2>

<p>
The following program is supposed to generate two random 8-bit integer and print
them on stdout:
</p>

<pre>

#include &lt;stdio.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;

/* Returns -1 if error, other number if ok. */
int get_random_chars(char *r1, char*r2)
{
	int f = open("/dev/urandom", O_RDONLY);

	if (f &lt; 0)
		return -1;
	if (read(f, r1, sizeof(*r1)) &lt; 0)
		return -1;
	if (read(f, r2, sizeof(*r2)) &lt; 0)
		return -1;
	close(f);

	return *r1 &amp; *r2;
}

int main(void)
{
	char r1;
	char r2;
	int ret;

	ret = get_random_chars(&amp;r1, &amp;r2);

	if (ret &lt; 0)
		fprintf(stderr, "error");
	else
		printf("%d %d\n", r1, r2);

	return ret &lt; 0;
}

</pre>

<p>
On my architecture (Linux on IA-32) it has a bug that makes it print "error"
instead of the numbers sometimes.
</p>

</section>
</section>
<section>
<h1>The Model</h1>

<p>
Every time we run the program, the bug can either show up or not. It has a
non-deterministic behaviour that requires statistical analysis.
</p>
<p>
We will model a single program run as a
<a href="https://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli trial</a>, with success
defined as "seeing the bug", as that is the event we are interested in. We have
the following parameters when using this model:
</p>

<ul>
<li>\(n\): the number of tests made;
</li>
<li>\(k\): the number of times the bug was observed in the \(n\) tests;
</li>
<li>\(p\): the unknown (and, most of the time, unknowable) probability of seeing
  the bug.
</li>
</ul>

<p>
As a Bernoulli trial, the number of errors \(k\) of running the program \(n\)
times follows a
<a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>
\(k \sim B(n,p)\). We will use this model to estimate \(p\) and to confirm the
hypotheses that the bug no longer exists, after fixing the bug in whichever
way we can.
</p>
<p>
By using this model we are implicitly assuming that all our tests are performed
independently and identically. In order words: if the bug happens more ofter in
one environment, we either test always in that environment or never; if the bug
gets more and more frequent the longer the computer is running, we reset the
computer after each trial. If we don't do that, we are effectively estimating
the value of \(p\) with trials from different experiments, while in truth each
experiment has its own \(p\). We will find a single value anyway, but it has no
meaning and can lead us to wrong conclusions.
</p>

<section>
<h2>Physical analogy</h2>

<p>
Another way of thinking about the model and the strategy is by creating a
physical analogy with a box that has an unknown number of green and red balls:
</p>

<ul>
<li>Bernoulli trial: taking a single ball out of the box and looking at its
  color - if it is red, we have observed the bug, otherwise we haven't. We then
  put the ball back in the box.
</li>
<li>\(n\): the total number of trials we have performed.
</li>
<li>\(k\): the total number of red balls seen.
</li>
<li>\(p\): the total number of red balls in the box divided by the total number of
  green balls in the box.
</li>
</ul>

<p>
Some things become clearer when we think about this analogy:
</p>

<ul>
<li>If we open the box and count the balls, we can know \(p\), in contrast with
  our original problem.
</li>
<li>Without opening the box, we can estimate \(p\) by repeating the trial. As
  \(n\) increases, our estimate for \(p\) improves. Mathematically:
  \[p = \lim_{n\to\infty}\frac{k}{n}\]
</li>
<li>Performing the trials in different conditions is like taking balls out of
  several different boxes. The results tell us nothing about any single box.
</li>
</ul>

<p>
 <img class="img-responsive" class="center" src="boxballs.png" alt=""> 
</p>

</section>
</section>
<section>
<h1>Estimating \(p\)</h1>

<p>
Before we try fixing anything, we have to know more about the bug, starting by
the probability \(p\) of reproducing it. We can estimate this probability by
dividing the number of times we see the bug \(k\) by the number of times we
tested for it \(n\). Let's try that with our sample bug:
</p>

<pre>
$ ./hasbug
67 -68
$ ./hasbug
79 -101
$ ./hasbug
error
</pre>

<p>
We know from the source code that \(p=25%\), but let's pretend that we don't, as
will be the case with practically every non-deterministic bug. We tested 3
times, so \(k=1, n=3 \Rightarrow p \sim 33%\), right? It would be better if we
tested more, but how much more, and exactly what would be better?
</p>

<section>
<h2>\(p\) precision</h2>

<p>
Let's go back to our box analogy: imagine that there are 4 balls in the box, one
red and three green. That means that \(p = 1/4\). What are the possible results
when we test three times?
</p>

<table class="table table-bordered">
<tr>
<th>Red balls</th>
<th>Green balls</th>
<th>\(p\) estimate</th>
</tr>
<tr>
<td>0</td>
<td>3</td>
<td>0%</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>33%</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>66%</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>100%</td>
</tr>
</table>

<p>
The less we test, the smaller our precision is. Roughly, \(p\) precision will
be at most \(1/n\) - in this case, 33%. That's the step of values we can find
for \(p\), and the minimal value for it.
</p>
<p>
Testing more improves the precision of our estimate.
</p>

</section>
<section>
<h2>\(p\) likelihood</h2>

<p>
Let's now approach the problem from another angle: if \(p = 1/4\), what are the
odds of seeing one error in four tests? Let's name the 4 balls as 0-red,
1-green, 2-green and 3-green:
</p>
<p>
<iframe src="r1w3_n4_results.html" style="width:100%;height:500px;"></iframe>
</p>
<p>
The table above has all the possible results for getting 4 balls out of the
box. That's \(4^4=256\) rows, generated by <a href="http://www.lpenz.org/articles/bugprobhunt/box">this</a> python script.
The same script counts the number of red balls in each row, and outputs the
following table:
</p>

<table class="table table-bordered">
<tr>
<th>k</th>
<th>rows</th>
<th>%</th>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>0.39%</td>
</tr>
<tr>
<td>3</td>
<td>12</td>
<td>4.69%</td>
</tr>
<tr>
<td>2</td>
<td>54</td>
<td>21.09%</td>
</tr>
<tr>
<td>1</td>
<td>108</td>
<td>42.19%</td>
</tr>
<tr>
<td>0</td>
<td>81</td>
<td>31.64%</td>
</tr>
</table>

<p>
That means that, for \(p=1/4\), we see 1 red ball and 3 green balls only 42% of
the time when getting out 4 balls.
</p>
<p>
What if \(p = 1/3\) - one red ball and two green balls? We would get the
following table:
</p>

<table class="table table-bordered">
<tr>
<th>k</th>
<th>rows</th>
<th>%</th>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1.23%</td>
</tr>
<tr>
<td>3</td>
<td>8</td>
<td>9.88%</td>
</tr>
<tr>
<td>2</td>
<td>24</td>
<td>29.63%</td>
</tr>
<tr>
<td>1</td>
<td>32</td>
<td>39.51%</td>
</tr>
<tr>
<td>0</td>
<td>16</td>
<td>19.75%</td>
</tr>
</table>

<p>
What about \(p = 1/2\)?
</p>

<table class="table table-bordered">
<tr>
<th>k</th>
<th>rows</th>
<th>%</th>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>6.25%</td>
</tr>
<tr>
<td>3</td>
<td>4</td>
<td>25.00%</td>
</tr>
<tr>
<td>2</td>
<td>6</td>
<td>37.50%</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td>25.00%</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>6.25%</td>
</tr>
</table>

<p>
So, let's assume that you've seen the bug once in 4 trials. What is the value of
\(p\)? You know that can happen 42% of the time if \(p=1/4\), but you also know
it can happen 39% of the time if \(p=1/3\), and 25% of the time if \(p=1/2\).
Which one is it?
</p>
<p>
The graph bellow shows the discrete likelihood for all \(p\) percentual values
for getting 1 red and 3 green balls:
</p>
<p>
 <img class="img-responsive" class="center" src="r1w3_dist.png" alt=""> 
</p>
<p>
The fact is that, <em>given the data</em>, the estimate for \(p\)
follows a <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta distribution</a>
\(Beta(k+1, n-k+1) = Beta(2, 4)\)
(<a href="http://stats.stackexchange.com/questions/13225/what-is-the-distribution-of-the-binomial-distribution-parameter-p-given-a-samp">1</a>)
The graph below shows the probability distribution density of \(p\):
</p>
<p>
 <img class="img-responsive" class="center" src="r1w3_dens.png" alt=""> 
</p>
<p>
The R script used to generate the first plot is <a href="http://www.lpenz.org/articles/bugprobhunt/pdistplot.R">here</a>, the
one used for the second plot is <a href="http://www.lpenz.org/articles/bugprobhunt/pdensplot.R">here</a>.
</p>

</section>
<section>
<h2>Increasing \(n\), narrowing down the interval</h2>

<p>
What happens when we test more? We obviously increase our precision, as it is at
most \(1/n\), as we said before - there is no way to estimate that \(p=1/3\) when we
only test twice. But there is also another effect: the distribution for \(p\)
gets taller and narrower around the observed ratio \(k/n\):
</p>
<p>
 <img class="img-responsive" class="center" src="pdens_many.png" alt=""> 
</p>

</section>
<section>
<h2>Investigation framework</h2>

<p>
So, which value will we use for \(p\)?
</p>

<ul>
<li>The smaller the value of \(p\), the more we have to test to reach a given
  confidence in the bug solution.
</li>
<li>We must, then, choose the probability of error that we want to tolerate, and
  take the <em>smallest</em> value of \(p\) that we can.
<p></p>
  A usual value for the probability of error is 5% (2.5% on each side).
</li>
<li>That means that we take the value of \(p\) that leaves 2.5% of the area of the
  density curve out on the left side. Let's call this value
  \(p_{min}\).
</li>
<li>That way, if the observed \(k/n\) remains somewhat constant,
  \(p_{min}\) will raise, converging to the "real" \(p\) value.
</li>
<li>As \(p_{min}\) raises, the amount of testing we have to do after fixing the
  bug decreases.
</li>
</ul>

<p>
By using this framework we have direct, visual and tangible incentives to test
more. We can objectively measure the potential contribution of each test.
</p>
<p>
In order to calculate \(p_{min}\) with the mentioned properties, we have
to solve the following equation:
</p>
<p>
\[\sum_{k=0}^{k}{n\choose{k}}p_{min} ^k(1-p_{min})^{n-k}=\frac{\alpha}{2} \]
</p>
<p>
\(alpha\) here is twice the error we want to tolerate: 5% for an error of 2.5%.
</p>
<p>
That's not a trivial equation to solve for \(p_{min}\). Fortunately, that's
the formula for the confidence interval of the binomial distribution, and there
are a lot of sites that can calculate it:
</p>

<ul>
<li><a href="http://statpages.info/confint.html">http://statpages.info/confint.html</a>: \(\alpha\) here is 5%.
</li>
<li><a href="http://www.danielsoper.com/statcalc3/calc.aspx?id=85:">http://www.danielsoper.com/statcalc3/calc.aspx?id=85:</a> results for \(\alpha\)
  1%, 5% and 10%.
</li>
<li><a href="https://www.google.com.br/search?q=binomial+confidence+interval+calculator:">https://www.google.com.br/search?q=binomial+confidence+interval+calculator:</a>
  google search.
</li>
</ul>

</section>
</section>
<section>
<h1>Is the bug fixed?</h1>

<p>
So, you have tested a lot and calculated \(p_{min}\). The next step is fixing
the bug.
</p>
<p>
After fixing the bug, you will want to test again, in order to
confirm that the bug is fixed. How much testing is enough testing?
</p>
<p>
Let's say that \(t\) is the number of times we test the bug after it is fixed.
Then, if our fix is not effective and the bug still presents itself with
a probability greater than the \(p_{min}\) that we calculated, the probability
of <em>not</em> seeing the bug after \(t\) tests is:
</p>
<p>
\[\alpha = (1-p_{min})^t \]
</p>
<p>
Here, \(\alpha\) is also the probability of making a
<a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">type I error</a>,
while \(1 - \alpha\) is the <em>statistical significance</em> of our tests.
</p>
<p>
We now have two options:
</p>

<ul>
<li>arbitrarily determining a standard statistical significance and testing enough
  times to assert it.
</li>
<li>test as much as we can and report the achieved statistical significance.
</li>
</ul>

<p>
Both options are valid. The first one is not always feasible, as the cost of
each trial can be high in time and/or other kind of resources.
</p>
<p>
The standard statistical significance in the industry is 5%, we recommend either
that or less.
</p>
<p>
Formally, this is very similar to a
<a href="https://en.wikipedia.org/wiki/Hypothesis_testing">statistical hypothesis testing</a>.
</p>

</section>
<section>
<h1>Back to the Bug</h1>

<section>
<h2>Testing 20 times</h2>

<p>
<a href="trials.csv">This file</a> has the results found after running our program 5000
times. We must never throw out data, but let's pretend that we have tested our
program only 20 times. The observed \(k/n\) ration and the calculated
\(p_{min}\) evolved as shown in the following graph:
</p>
<p>
 <img class="img-responsive" class="center" src="trials20.png" alt=""> 
</p>
<p>
After those 20 tests, our \(p_{min}\) is about 12%.
</p>
<p>
Suppose that we fix the bug and test it again. The following graph shows the
statistical significance corresponding to the number of tests we do:
</p>
<p>
 <img class="img-responsive" class="center" src="after20.png" alt=""> 
</p>
<p>
In words: we have to test 24 times after fixing the bug to reach 95% statistical
significance, and 35 to reach 99%.
</p>
<p>
Now, what happens if we test more before fixing the bug?
</p>

</section>
<section>
<h2>Testing 5000 times</h2>

<p>
Let's now use all the results and assume that we tested 5000 times before fixing
the bug. The graph bellow shows \(k/n\) and \(p_{min}\):
</p>
<p>
 <img class="img-responsive" class="center" src="trials5000.png" alt=""> 
</p>
<p>
After those 5000 tests, our \(p_{min}\) is about 23% - much closer
to the real \(p\).
</p>
<p>
The following graph shows the statistical significance corresponding to the
number of tests we do after fixing the bug:
</p>
<p>
 <img class="img-responsive" class="center" src="after5000.png" alt=""> 
</p>
<p>
We can see in that graph that after about 11 tests we reach 95%, and after about
16 we get to 99%. As we have tested more before fixing the bug, we found a
higher \(p_{min}\), and that allowed us to test less after fixing the
bug.
</p>

</section>
</section>
<section>
<h1>Optimal testing</h1>

<p>
We have seen that we decrease \(t\) as we increase \(n\), as that can
potentially increases our lower estimate for \(p\). Of course, that value can
decrease as we test, but that means that we "got lucky" in the first trials and
we are getting to know the bug better - the estimate is approaching the real
value in a non-deterministic way, after all.
</p>
<p>
But, how much should we test before fixing the bug? Which value is an ideal
value for \(n\)?
</p>
<p>
To define an optimal value for \(n\), we will minimize the sum \(n+t\). This
objective gives us the benefit of minimizing the total amount of testing without
compromising our guarantees. Minimizing the testing can be fundamental if each
test costs significant time and/or resources.
</p>
<p>
The graph bellow shows us the evolution of the value of \(t\) and \(t+n\) using
the data we generated for our bug:
</p>
<p>
 <img class="img-responsive" class="center" src="tbyn.png" alt=""> 
</p>
<p>
We can see clearly that there are some low values of \(n\) and \(t\) that give
us the guarantees we need. Those values are \(n = 15\) and \(t = 24\), which
gives us \(t+n = 39\).
</p>
<p>
While you can use this technique to minimize the total number of tests performed
(even more so when testing is expensive), testing more is always a good thing,
as it always improves our guarantee, be it in \(n\) by providing us with a
better \(p\) or in \(t\) by increasing the statistical significance of the
conclusion that the bug is fixed. So, before fixing the bug, test until you see
the bug at least once, and then at least the amount specified by this
technique - but also test more if you can, there is no upper bound, specially
after fixing the bug. You can then report a higher confidence in the solution.
</p>

</section>
<section>
<h1>Conclusions</h1>

<p>
When a programmer finds a bug that behaves in a non-deterministic way, he
knows he should test enough to know more about the bug, and then even more
after fixing it. In this article we have presented a framework that provides
criteria to define numerically how much testing is "enough" and "even more." The
same technique also provides a method to objectively measure the guarantee that
the amount of testing performed provides, when it is not possible to test
"enough."
</p>
<p>
We have also provided a real example (even though the bug itself is artificial)
where the framework is applied.
</p>
<p>
As usual, the source code of this page (R scripts, etc) can be found and
downloaded in <a href="https://github.com/lpenz/lpenz.github.io">https://github.com/lpenz/lpenz.github.io</a>
</p>
</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Creating a pmount-compatible encrypted USB drive</title>
			<link>http://www.lpenz.org/articles/cryptusb</link>
			<guid>http://www.lpenz.org/articles/cryptusb</guid>
			<pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
			<description><![CDATA[]]></description>
		</item>

	
		<item>
			<title>Debianization with git-buildpackage</title>
			<link>http://www.lpenz.org/articles/debgit</link>
			<guid>http://www.lpenz.org/articles/debgit</guid>
			<pubDate>Sun, 11 Apr 2010 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
<strong>Updated 2018-04-03</strong>: <em>git-buildpackage</em>'s commands have changed, so this
article had to be fixed; I took the opportunity to improve a few things as
well.
</p>
<p>
After building some useful piece of software, one has to decide how to best
deploy it. In UNIX, the standard way to do that is by publishing the source
code in .tar.gz format and requiring users to compile it.
</p>
<p>
In Debian there is an alternative: using a .deb package. With a .deb package, a
single <code>dpkg -i ${PACKAGE}.deb</code> installs the software.
</p>
<p>
This article explains how to create and support a .deb package for a simple
software maintained in git, by tracking the packaging scheme in a specific
branch on the same repository.
</p>

<section>
<h1>Prerequisites</h1>

<p>
In order to ease the packaging and keep our package warning-free, it should
have in its main repository:
</p>

<ul>
<li>An <code>AUTHORS</code> file with copyright information.
</li>
<li>A manual: <code>${PACKAGE}.1</code> or similar.
</li>
<li>A <code>COPYING</code> file with GPL information or some other license.
</li>
<li>An appropriate build file for the package. For C/C++ programs, I recommend
  using <code>cmake</code>; for python, <code>setup.py</code>, etc.
</li>
</ul>

<p>
These items are not debian-specific and are useful for everyone.
</p>

</section>
<section>
<h1>Initial packaging setup</h1>

<p>
The first step is creating the <code>${PACKAGE}_${VERSION}.orig.tar.gz</code> file. You
can use git itself for that, by running the following commands in the
repository:
</p>

<pre>
PREFIX=${PACKAGE}_${VERSION}
git archive --format=tar --prefix=$PREFIX/ $VERSION | gzip -c &gt; ../$PREFIX.orig.tar.gz
</pre>

<p>
You can check the contents of the archive with <em>tar</em>. If there are extraneous
files in the archive, you can configure git-archove to exclude them by creating
a <code>.gitattributes</code> file; for example:
</p>

<pre>
.gitignore      export-ignore
.gitattributes  export-ignore
.travis.yml     export-ignore
</pre>

<p>
The next step is to create the debian branches in the git repository: on the
debian-upstream branch, we store the upstream source, while the debian-debian
branch holds the debian package data.
This separation provides a cleaner revision history by separating the changes
that affect the software from the changes in the packaging.
</p>
<p>
In order to create these branches, we issue the following commands in the git
repository:
</p>

<pre>
git checkout --orphan debian-upstream
git rm --cached -r .
git clean -xfd
git commit --allow-empty -m 'Start of debian branches.'
git checkout -b debian-debian
</pre>

<p>
That creates both branches as orphans, pointing to an empty root commit.
</p>
<p>
We now use the <code>../${PACKAGE}_${VERSION}.orig.tar.gz</code> file to create the
initial <code>debian</code> directory in the debian-debian branch:
</p>

<pre>
dh_make -s -p ${PACKAGE}_${VERSION}
</pre>

<p>
We can now customize the standard <code>debian</code> directory created. You must edit
the following files: <code>changelog</code>, <code>control</code>, <code>copyright</code> and <code>rules</code>.
Besides those, the <code>compat</code> file must be present; the other files can be
safely removed.
</p>
<p>
After changing the files that <code>dh_make</code> created, you should create a
<code>debian/gbp.conf</code> with the following contents:
</p>

<pre>
[DEFAULT]
upstream-branch=debian-upstream
debian-branch=debian-debian
</pre>

<p>
We can now commit the debian directory in the debian-debian branch.
</p>

</section>
<section>
<h1>Importing the sources</h1>

<p>
In the debian-debian branch:
</p>

<pre>
gbp import-orig --no-interactive ../${PACKAGE}_${VERSION}.orig.tar.gz
</pre>

<p>
That imports the original sources to the debian-upstream branch, and merge
it into the debian-debian branch.
</p>

</section>
<section>
<h1>Creating the package</h1>

<p>
To create the debian package:
</p>

<pre>
gbp buildpackage -us -uc --git-tag
</pre>

</section>
<section>
<h1>Importing further versions</h1>

<p>
Create the new <code>../${PACKAGE}_${VERSION}/.orig.tar.gz</code> and then:
</p>

<pre>
gbp import-orig --no-interactive ../${PACKAGE}_${VERSION}.orig.tar.gz
</pre>

<p>
Edit the <code>debian/changelog</code> file (we can use <code>dch -i -v $VERSION</code> for
that), and create a new package:
</p>

<pre>
gbp buildpackage -us -uc --git-tag
</pre>

<p>
Yes, it's that easy.
</p>

</section>
<section>
<h1>Final remarks</h1>

<p>
After an initial expensive setup, package creation of further versions is
mostly painless, which is the whole point of git-buildpackage and friends.
</p>
<p>
Besides this article, we should check the <code>debian</code> dir of some already packaged
software for reference. We can look at the
<a href="https://github.com/lpenz/execpermfix">execpermfix</a> repository at
<a href="https://github.com">github</a> when first trying to package something.
</p>
<p>
Further information:
</p>

<ul>
<li><a href="https://www.eyrie.org/~eagle/notes/debian/git.html">https://www.eyrie.org/~eagle/notes/debian/git.html</a>
</li>
<li><a href="http://honk.sigxcpu.org/projects/git-buildpackage/manual-html/gbp.html">http://honk.sigxcpu.org/projects/git-buildpackage/manual-html/gbp.html</a>
</li>
<li><a href="http://www.debian-administration.org/article/Rolling_your_own_Debian_packages_part_1">http://www.debian-administration.org/article/Rolling_your_own_Debian_packages_part_1</a>
</li>
</ul>

</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Hard drive occupation prediction with R - The linear regression</title>
			<link>http://www.lpenz.org/articles/df0pred-1</link>
			<guid>http://www.lpenz.org/articles/df0pred-1</guid>
			<pubDate>Sun, 15 Aug 2010 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
On some environments, disk space usage can be pretty predictable. In this post,
we will see how to do a linear regression to estimate when free space will reach
zero, and how to assess the quality of such regression, all using
<a href="https://en.wikipedia.org/wiki/R_programming_language">R</a> - the
statistical software environment.
</p>

<section>
<h1>Prerequisites</h1>

<p>
The first thing we need is the data. By running a simple
<code>(date --utc; df -k; echo) &gt;&gt; /var/dflog.txt</code>
everyday at 00:00 by cron, we will have more than enough, as that will store the
date along with total, free and used space for all mounted devices.
</p>
<p>
On the other hand, that is not really easy to parse in R, unless we learn more
about the language. In order to keep this post short, we invite the reader to
use his favorite scripting language (or python) to process that into a file with
the day in the first column and the occupied space in the second, and a row for
each day:
</p>

<pre>
YYYY-MM-DD free space
YYYY-MM-DD free space
(...)
</pre>

<p>
This format can be read and parsed in R with a single command.
</p>
<p>
<a href="http://www.lpenz.org/articles/df0pred-1/duinfo.dat">This</a> is the data file we will use as source for the results
provided in this article. Feel free to download it and repeat the process.
All number in the file are in MB units, and we assume an HD of 500GB. We will
call the date the free space reaches 0 as the <strong>df0</strong>.
</p>

</section>
<section>
<h1>Starting up</h1>

<p>
After running <strong>R</strong> in the shell prompt, we get the usual license and basic help
information.
</p>
<p>
The first step is to import the data:
</p>

<pre>
&gt; duinfo &lt;- read.table('duinfo.dat', colClasses=c("Date","numeric"), col.names=c("day","usd"))
&gt; attach(duinfo)
&gt; totalspace &lt;- 500000
</pre>

<p>
The variable <em>duinfo</em> is now a list with two columns: <em>day</em> and <em>usd</em>. The
<code>attach</code> command allows us to use the column names directly. The
<em>totalspace</em> variable is there just for clarity in the code.
</p>
<p>
We can check the data graphically by issuing:
</p>

<pre>
&gt; plot(usd ~ day, xaxt='n')
&gt; axis.Date(1, day, format='%F')
</pre>

<p>
That gives us an idea on how predictable the usage of our hard drive is.
</p>
<p>
From our example, we get:
</p>
<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-1/pointplot.png" alt=""> 
</p>

</section>
<section>
<h1>Linear model</h1>

<p>
We can now create and take a look at our linear model object:
</p>

<pre>
&gt; model &lt;- lm(usd ~ day)
&gt; model
</pre>

<pre>

Call:
lm(formula = usd ~ day)

Coefficients:
(Intercept)          day  
 -6424661.2        466.7  

</pre>

<p>
The second coefficient in the example tells us that we are consuming about 559 MB of disk space per day.
</p>
<p>
We can also plot the linear model over our data:
</p>

<pre>
&gt; abline(model)
</pre>

<p>
The example plot, with the line:
</p>
<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-1/lmplot.png" alt=""> 
</p>

</section>
<section>
<h1>Evaluating the model</h1>

<p>
R provides us with a very generic command that generates statistical information
about objects: <strong>summary</strong>. Let's use it on our linear model objects:
</p>

<pre>
&gt; summary(model)
</pre>

<pre>

Call:
lm(formula = usd ~ day)

Residuals:
    Min      1Q  Median      3Q     Max 
-3612.1 -1412.8   300.7  1278.9  3301.0 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.425e+06  3.904e+04  -164.6   &lt;2e-16 ***
day          4.667e+02  2.686e+00   173.7   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1697 on 161 degrees of freedom
Multiple R-squared:  0.9947,	Adjusted R-squared:  0.9947 
F-statistic: 3.019e+04 on 1 and 161 DF,  p-value: &lt; 2.2e-16

</pre>

<p>
To check the quality of a linear regression, we focus on the <strong>residuals</strong>, as
they represent the error of our model. We calculate them by subtracting the
expected value (from the model) from the sampled value, for every sample.
</p>
<p>
Let's see what each piece of information above means: the first is the
<a href="https://en.wikipedia.org/wiki/Five-number_summary">five-number summary</a>
of the residuals. That tells us the maximum and minimum error, and that 75% of
the errors are between -1.4 GB and 1.3 GB. We then get the results of a
<a href="https://en.wikipedia.org/wiki/Student%27s_t-test">Student's t-test</a> of
the model coefficients against the data. The last column tells us roughly how
probable seeing the given residuals is, assuming that the disk space does not
depend on the date - it's the
<a href="https://en.wikipedia.org/wiki/P-value">p-value</a>. We usually accept an
hypothesis when the p-value is less than 5%; in this example, we have a large
margin for both coefficients. The last three lines of the summary give us more
measures of fit: the
<a href="https://en.wikipedia.org/wiki/R-squared">r-squared</a> values - the closest
to 1, the better; and the general p-value from the f-statistics, less than 5%
again.
</p>
<p>
In order to show how bad a linear model can be, the summary bellow was generated
by using 50GB as the disk space and adding a random value between -1GB and 1GB
each day:
</p>

<pre>

Call:
lm(formula = drand$usd ~ drand$day)

Residuals:
     Min       1Q   Median       3Q      Max 
-1012.97  -442.62   -96.19   532.27  1025.01 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 17977.185  33351.017   0.539    0.591
drand$day       2.228      2.323   0.959    0.340

Residual standard error: 589.7 on 84 degrees of freedom
Multiple R-squared:  0.01083,	Adjusted R-squared:  -0.0009487 
F-statistic: 0.9194 on 1 and 84 DF,  p-value: 0.3404

</pre>

<p>
It's easy to notice that, even though the five-number summary is narrower, the
p-values are greater than 5%, and the r-squared values are very far from 1. That
happened because the residuals are not normally distributed.
</p>
<p>
Now that we are (hopefully) convinced that our linear model fits our data
well, we can use it to predict hard-disk shortage.
</p>

</section>
<section>
<h1>Predicting disk-free-zero</h1>

<p>
Until now, we represented disk space as a function of time, creating a model
that allows us to predict the used disk space given the date. But what we really
want now is to predict the date our disk will be full. In order to do that, we
have to invert the model. Fortunately, all statistical properties (t-tests,
f-statistics) hold in the inverted model.
</p>

<pre>
&gt; model2 &lt;- lm(day ~ usd)
</pre>

<p>
We now use the <strong>predict</strong> function to extrapolate the model.
</p>

<pre>
&gt; predict(model2, data.frame(usd = totalspace))
       1 
14837.44 
</pre>

<p>
But... when is that? Well, that is the numeric representation of a day in R:
the number of days since 1970-01-01. To get the human-readable day, we
use:
</p>

<pre>
&gt; as.Date(predict(model2, data.frame(usd = totalspace)), origin="1970-01-01")
           1 
"2010-08-16" 
</pre>

<p>
There we are: df0 will be at the above date <strong>if</strong> the
current pattern holds until then.
</p>

</section>
<section>
<h1>Conclusion</h1>

<p>
The linear model can give us the predicted hard disk space usage at any future
date, as long as collected data pattern <strong>is linear</strong>. If the data we collected
has a break point - some disk cleanup or software installation - the model will
not give good results. We will usually see that in the analysis, but we should
also always look at the graph.
</p>
<p>
This article is focused on teaching R basics - data input and plotting. We skip
most of the formalities of science here, and linear regression is certainly not
a proper df0 prediction method in the general case.
</p>
<p>
On the other hand, in the <a href="http://www.lpenz.org/articles/df0pred-1/../df0pred-2/index.html">next part</a> of this
article we will see a more robust method for df0 prediction. We will also
sacrifice our ability to see the used space vs time to get a
statistical distribution for the date of exhaustion, which is a lot more useful
in general.
</p>

</section>
<section>
<h1>Further reading</h1>

<ul>
<li><a href="http://www.cyclismo.org/tutorial/R/index.html">http://www.cyclismo.org/tutorial/R/index.html</a>: R tutorial
</li>
<li><a href="http://www.r-tutor.com/">http://www.r-tutor.com/</a>: An R introduction to statistics
</li>
<li><a href="https://www.datacamp.com/courses/free-introduction-to-r">https://www.datacamp.com/courses/free-introduction-to-r</a>: Datacamp's
  Introduction to R course
</li>
<li><a href="http://cran.r-project.org/doc/contrib/Lemon-kickstart/index.html">http://cran.r-project.org/doc/contrib/Lemon-kickstart/index.html</a>: Kickstarting R
</li>
<li><a href="http://data.princeton.edu/R/linearModels.html">http://data.princeton.edu/R/linearModels.html</a>: "Linear models" page of
  Introduction to R.
</li>
<li><a href="http://www.r-bloggers.com/">http://www.r-bloggers.com/</a>: daily news and tutorials about R, very good to
  learn the language and see what people are doing with it.
</li>
</ul>

</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Hard drive occupation prediction with R - part 2 - Getting the probability distribution</title>
			<link>http://www.lpenz.org/articles/df0pred-2</link>
			<guid>http://www.lpenz.org/articles/df0pred-2</guid>
			<pubDate>Sat, 22 Jan 2011 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
On the <a href="http://www.lpenz.org/articles/df0pred-2/../df0pred-1/index.html">first</a> article, we saw a quick-and-dirty method to
predict disk space exhaustion when the usage pattern is rigorously linear. We did that by
importing our data into <a href="https://en.wikipedia.org/wiki/R_programming_language">R</a>
and making a linear regression.
</p>
<p>
In this article we will see the problems with that method, and deploy a
more robust solution. Besides robustness, we will also see how we can generate a
probability distribution for the date of disk space exhaustion instead of
calculating a single day.
</p>

<section>
<h1>The problem with the linear regression</h1>

<p>
The linear regression used in the first article has a serious
lack of <a href="https://en.wikipedia.org/wiki/Robust_statistics">robustness</a>.
That means that it is very sensitive to even single departures
from the linear pattern. For instance, if we periodically delete some big
files in the hard disk, we end up breaking the sample in parts that cannot be
analysed together. If we plot the line given by the linear model, we can see
clearly that it does not fit our overall data very well:
</p>
<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-2/lm.png" alt=""> 
</p>
<p>
(<a href="http://www.lpenz.org/articles/df0pred-2/duinfospike.dat">Data file</a>)
</p>
<p>
We can see in the graph that the linear model gives us a line that our free disk
space is increasing instead of decreasing! If we use this model, we will reach
the conclusion that we will never reach df0.
</p>
<p>
If we keep analysing used disk space, there is not much we can do besides
discarding the data gathered before the last cleanup. There is no way to easily
ignore only the cleanup.
</p>
<p>
In fact, we can only use the linear regression method when our disk consumption
pattern is linear for the analysed period - and that rarely is the case
when there is human intervention. We should always look at the graph to see if
the model makes sense.
</p>

</section>
<section>
<h1>A na&iuml;ve new method: averaging the difference</h1>

<p>
Instead of using the daily used disk space as input, we will use the
daily <strong>difference</strong> (or delta) of used disk space. By itself, this reduces a
big disk cleanup to a single outlier instead of breaking our sample. We could
then just filter out the outliers, calculate the average daily increment in used
disk space and divide the current free space by it. That would give us the
average number of days left until disk exhaustion. Well, that would also give us
some new problems to solve.
</p>
<p>
The first problem is that filtering out the outliers is neither
straightforward nor recommended. Afterall, we are throwing out data that might
be meaningful: it could be a regular monthly process that we should take into
account to generate a better prediction.
</p>
<p>
Besides, by averaging disk consumption and dividing free disk space by it,  we
would still not have the probability distribution for the date, only a single
value.
</p>

</section>
<section>
<h1>The real new method: days left by Monte Carlo simulation</h1>

<p>
Instead of calculating the number of days left from the data, we will use a
technique called <a href="https://en.wikipedia.org/wiki/Monte_carlo_simulation">Monte Carlo simulation</a>
to generate the distribution of days left. The idea is simple: we sample the
data we have - daily used disk space - until the sum is above the free disk
space; the number of samples taken is the number of days left. By doing that
repeatedly, we get the set of "possible days left" with a distribution that
corresponds to the data we have collected. Let's how we can do that in R.
</p>
<p>
First, let's load the data file that we will use (same one used in the
introduction) along with a variable that holds the size of the disk (500GB; all
units are in MB):
</p>

<pre>

duinfo &lt;- read.table('duinfospike.dat',
		colClasses=c("Date","numeric"),
		col.names=c("day","usd"))
attach(duinfo)
totalspace &lt;- 500000
today &lt;- tail(day, 1)

</pre>

<p>
We now get the delta of the disk usage. Let's take a look at it:
</p>

<pre>
dudelta &lt;- diff(usd)
</pre>

<pre>
plot(dudelta, xaxt='n', xlab='')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-2/delta.png" alt=""> 
</p>
<p>
The summary function gives us the five-number summary, while the boxplot shows
us how the data is distributed graphically:
</p>

<pre>
summary(dudelta)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-29583.00      5.25    301.00    123.37    713.00   4136.00 
</pre>

<pre>
boxplot(dudelta)
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-2/deltabox.png" alt=""> 
</p>
<p>
The kernel density plot gives us about the same, but in another visual format:
</p>

<pre>
plot(density(dudelta))
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-2/deltakd.png" alt=""> 
</p>
<p>
We can see the cleanups right there, as the lower points.
</p>
<p>
The next step is the creation of the sample of the number of days left until
exhaustion. In order to do that, we create an R function that sums values taken
randomly from our delta sample until our free space zeroes, and returns the
number of samples taken:
</p>

<pre>

f &lt;- function(spaceleft) {
    days &lt;- 0
    while(spaceleft &gt; 0) {
        days &lt;- days + 1
        spaceleft &lt;- spaceleft - sample(dudelta, 1, replace=TRUE)
    }
    days
}

</pre>

<p>
By repeatedly running this function and gathering the results, we generate a set
of number-of-days-until-exhaustion that is robust and corresponds to the data we
have observed. This robustness means that we don't even need to remove outliers,
as they will not disproportionally bias out results:
</p>

<pre>
freespace &lt;- totalspace - tail(usd, 1)
daysleft &lt;- replicate(5000, f(freespace))
</pre>

<pre>
plot(daysleft)
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-2/daysleft.png" alt=""> 
</p>
<p>
What we want now is the
<a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">empirical cumulative distribution</a>.
This function gives us the probability that we will reach df0 <strong>before</strong> the
given date.
</p>

<pre>
df0day &lt;- sort(daysleft + today)
df0ecdfunc &lt;- ecdf(df0day)
df0prob &lt;- df0ecdfunc(df0day)
</pre>

<pre>
plot(df0day, df0prob, xaxt='n', type='l')
axis.Date(1, df0day, at=seq(min(df0day), max(df0day), 'year'), format='%F')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-2/df0ecdf.png" alt=""> 
</p>
<p>
With the cumulative probability estimate, we can see when we have to start
worrying about the disk by looking at the first day that the probability of df0
is above 0:
</p>

<pre>
df0day[1]
[1] "2010-06-13"
df0ecdfunc(df0day[1])
[1] 2e-04
</pre>

<p>
Well, we can also be a bit more bold and wait until the chances of reaching df0
rise above 5%:
</p>

<pre>
df0day[which(df0prob &gt; 0.05)[1]]
[1] "2010-08-16"
</pre>

<p>
Mix and match and see what a good convention for your case is.
</p>

</section>
<section>
<h1>Conclusion</h1>

<p>
This and the <a href="http://www.lpenz.org/articles/df0pred-2/../df0pred-1/index.html">previous article</a> showed how to use
statistics in R to predict when free hard-disk space will zero.
</p>
<p>
The first article was main purpose was to serve as an introduction to R. There
are many reasons that make linear regression an unsuitable technique for
df0 prediction - the underlying process of disk consumption is certainly not
linear. But, if the graph shows you that the line fits, there is no reason to
ignore it.
</p>
<p>
Monte Carlo simulation, on the other hand, is a powerful and general technique.
It assumes little about the data (non-parameterized), and it can give you
probability distributions. If you want to forecast something, you can always
start recording data and use Monte Carlo in some way to make predictions
<strong>based on the evidence</strong>. Personally, I think we don't do this nearly as often
as we could. Well, <a href="http://www.joelonsoftware.com/items/2007/10/26.html">Joel is even using it to make schedules</a>.
</p>

</section>
<section>
<h1>Further reading</h1>

<ul>
<li><a href="http://www.joelonsoftware.com/items/2007/10/26.html">http://www.joelonsoftware.com/items/2007/10/26.html</a>: Joel's use of Monte Carlo
  to make schedules.
</li>
<li><a href="https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29">https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29</a>: Wikipedia's page
  on bootstrapping, which is clearer than the one on Monte Carlo simulations.
</li>
<li><a href="http://www.r-bloggers.com/">http://www.r-bloggers.com/</a>: daily news and tutorials about R, very good to
  learn the language and see what people are doing with it.
</li>
</ul>

</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Hard drive occupation prediction with R - part 3 - Predicting future ranges</title>
			<link>http://www.lpenz.org/articles/df0pred-3</link>
			<guid>http://www.lpenz.org/articles/df0pred-3</guid>
			<pubDate>Thu, 23 Jun 2011 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
On the <a href="http://www.lpenz.org/articles/df0pred-3/../df0pred-2/index.html">second</a> article, we saw how to use a Monte
Carlo simulation generate sample of disk space delta for future dates and
calculate the distribution probability of zeroing free space in the future.
</p>
<p>
In this article, we will see how we can plot the evolution of predicted
distribution for the occupied disk space. Instead of answering que question "how
likely is that my disk space will zero before date X?," we will answer
"how much disk space will I need by date X, and with what probability?"
</p>

<section>
<h1>The input data</h1>

<p>
<a href="http://www.lpenz.org/articles/df0pred-3/duinfospike.dat">This file</a> has the dataset we will use as example. It's
the same we used in the second part. The graph below shows it:
</p>
<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/usd.png" alt=""> 
</p>
<p>
We now import this data into R:
</p>

<pre>

duinfo &lt;- read.table('duinfospike.dat',
		colClasses=c("Date","numeric"),
		col.names=c("day","usd"))
attach(duinfo)
totalspace &lt;- 450000
today &lt;- tail(day, 1)

</pre>

<p>
We then build our simulations for the next 4 months:
</p>

<pre>
# Number of Monte Carlo samples
numsimulations &lt;- 10000

# Number of days to simulate
numdays    &lt;- 240

# Simulate:
simulate &lt;- function(data, ndays) {
	delta &lt;- diff(data)
	dssimtmp0 &lt;- replicate(numsimulations, tail(data, 1))
	dssimtmp  &lt;- dssimtmp0
	f &lt;- function(i) dssimtmp &lt;&lt;- dssimtmp + replicate(numsimulations, sample(delta, 1, replace=TRUE))
	cbind(dssimtmp0, mapply(f, seq(1, ndays)))
}
dssim &lt;- simulate(usd, numdays)

# Future days:
fday &lt;- seq(today, today+numdays, by='day')

</pre>

</section>
<section>
<h1>Visualizing the possible scenarios</h1>

<p>
What king of data have we built in our simulations? Each simulation is
built by sampling from the delta samples and adding to the current disk space
for each day in the simulated period. We can say that each individual simulation
is a possible scenario for the next 4 months. The graph bellow shows the
first 5 simulations:
</p>

<pre>
plot(fday, dssim[1,], ylim=c(min(dssim[1:5,]), max(dssim[1:5,])), ylab='usd', xlab='day', xaxt='n', type='l')
axis.Date(1, day, at=seq(min(fday), max(fday), 'week'), format='%F')
lines(fday, dssim[2,])
lines(fday, dssim[3,])
lines(fday, dssim[4,])
lines(fday, dssim[5,])
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcs3.png" alt=""> 
</p>
<p>
From this graph we can clearly see that the range of possible values for the
used disk space grows with time. All simulations start with the same value - the
used disk space for today - and grow apart as we sample from the delta pool.
</p>
<p>
We can also plot all simulations in a single graph:
</p>

<pre>
plot(fday, dssim[1,], ylim=c(min(dssim), max(dssim)), ylab='usd', xlab='', xaxt='n', type='l')
axis.Date(1, day, at=seq(min(fday), max(fday), 'week'), format='%F')
f &lt;- function(i) lines(fday, dssim[i,])
mapply(f, seq(2, numdays))
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcs.png" alt=""> 
</p>
<p>
This plot gives us an idea of the overall spread of the data, but it fails to
show the density. There are 10000 black lines there, with many of them
overlapping one another.
</p>

</section>
<section>
<h1>Visualizing the distribution for specific days</h1>

<p>
There is another way to look at our data: we have created, for each day, a
sample of the possible used disk spaces. We can take any day of the simulation
and look at the density:
</p>

<pre>
dssimchosen &lt;- list(density(dssim[,5]), density(dssim[,15]), density(dssim[,45]), density(dssim[,120]))
colors &lt;- rainbow(length(dssimchosen))
xs &lt;- c(mapply(function(d) d$x, dssimchosen))
ys &lt;- c(mapply(function(d) d$y, dssimchosen))
plot(dssimchosen[[1]], xlab='usd', ylab='dens',
	xlim=c(min(xs),max(xs)), ylim=c(min(ys),max(ys)), col=colors[1], main='')
lines(dssimchosen[[2]], col=colors[2])
lines(dssimchosen[[3]], col=colors[3])
lines(dssimchosen[[4]], col=colors[4])
abline(v=totalspace, col='gray')
legend('top', c('5 day', '15 days', '45 days', '120 days'), fill=colors)
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcsdaydens.png" alt=""> 
</p>
<p>
By looking at this graph we can see the trend:
</p>

<ul>
<li>The curves are getting flatter: we are getting more possible values for
  occupied disk space.
</li>
<li>The curves are moving to the right: we have more simulations with higher
  occupied disk space values.
</li>
</ul>

</section>
<section>
<h1>Visualizing the evolution of the distribution</h1>

<p>
So far, we have seen how we can visualize some simulations along the 4 months
and how we can visualize the distribution for some specific days.
</p>
<p>
We can also plot the distribution of the values for each day in the simulated 4
months. We can't use the kernel density plot or the histogram, as they use both
axes, but there are other options, most of them involving some abuse of the
built-in plot functions.
</p>

<section>
<h2>Boxplot</h2>

<p>
We can use the <em>boxplot</em> function to create a boxplot for each day in R in a
very straightforward way:
</p>

<pre>
boxplot(dssim, outline=F, names=seq(today, as.Date(today+numdays), by='day'), ylab='usd', xlab='day', xaxt='n')
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcsbox.png" alt=""> 
</p>
<p>
The boxplots glued together form a shape that shows us the distribution of our
simulations at any day:
</p>

<ul>
<li>The thick line in the middle of the graph is the median
</li>
<li>The darker area goes from the first quartile to the third - which means that
  50% of the samples are in that range
</li>
<li>The lighter area has the maximum and minimum points, if they are within 1.5
  <a href="https://en.wikipedia.org/wiki/Interquartile_range">IQR</a> of the upper/lower
  quartile.  Points out of this range are considered outliers and are not
  plotted.
</li>
</ul>

</section>
<section>
<h2>Quantile lines</h2>

<p>
We can use the <em>quantile</em> function to calculate the values of each
<a href="https://en.wikipedia.org/wiki/Quantile">quantile</a> per day, and plot the lines:
</p>

<pre>
q &lt;- 6
f &lt;- function(i) quantile(dssim[,i], seq(0, 1, 1.0/q))
qvals &lt;- mapply(f, seq(1, numdays+1))
colors &lt;- colorsDouble(rainbow, q+1)
plot(fday, qvals[1,], ylab='usd', xlab='day', xaxt='n', type='l', col=colors[1], ylim=c(min(qvals), max(qvals)))
mapply(function(i) lines(fday, qvals[i,], col=colors[i]), seq(2, q+1))
axis.Date(1, day, at=seq(min(fday), max(fday), 'week'), format='%F')
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcsquant.png" alt=""> 
</p>
<p>
The advantage of this type of graph over the boxplot is that it is parameterized
by <em>q</em>. This variable tells us the number of parts that we should divide our
sample in. The lines above show us the division. If <em>q</em> is odd, the middle
line is exactly the median. If <em>q</em> is 4, the lines will draw a shape similar
to that of the boxplot, the only difference being the top and bottom line, that
will include outliers - the boxplot filters outliers by using the IQR as
explained above.
</p>
<p>
In the code above, we have used the <em>colorsDouble</em> function to generate a
sequence of colors that folds in the middle:
</p>

<pre>
colorsDouble &lt;- function(colorfunc, numcolors) {
	colors0 &lt;- rev(colorfunc((1+numcolors)/2))
	c(colors0, rev(if (numcolors %% 2 == 0) colors0 else head(colors0, -1)))
}
</pre>

</section>
<section>
<h2>Quantile areas</h2>

<p>
We can also abuse the <em>barplots</em> function to create an area graph. We have to
eliminate the bar borders, zero the distance between them and plot a white bar
from the axis to the first quartile, if appropriate:
</p>

<pre>
q &lt;- 7
f &lt;- function(i) {
	qa &lt;- quantile(dssim[,i], seq(0, 1, 1.0/q))
	c(qa[1], diff(qa))
}
qvals &lt;- mapply(f, seq(1, numdays+1))
colors &lt;- c('white', colorsDouble(rainbow, q))
barplot(qvals, ylab='usd', xlab='day', col=colors, border=NA, space=0,
	names.arg=seq(min(fday), max(fday), 'day'), ylim=c(min(dssim), max(dssim)))
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcsquantbar.png" alt=""> 
</p>
<p>
In this case, using an odd <em>q</em> makes more sense, as we want to use the same
colors for the symmetric intervals. With an even <em>q</em>, there would either be a
larger middle interval with two quantiles or a broken symmetry. The code above
builds a larger middle interval when given an even <em>q</em>.
</p>

</section>
<section>
<h2>Quantile heat map</h2>

<p>
If we increase <em>q</em> and use <em>heat.colors</em> in a quantile area plot, we get
something similar to a heat map:
</p>

<pre>
q &lt;- 25
f &lt;- function(i) {
	qa &lt;- quantile(dssim[,i], seq(0, 1, 1.0/q))
	c(qa[1], mapply(function(j) qa[j] - qa[j-1], seq(2, q+1)))
}
qvals &lt;- mapply(f, seq(1, numdays+1))
colors &lt;- c('white', colorsDouble(heat.colors, q))
barplot(qvals, ylab='usd', xlab='day', col=colors, border=NA, space=0,
	names.arg=seq(min(fday), max(fday), 'day'), ylim=c(min(dssim), max(dssim)))
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcsquantheat.png" alt=""> 
</p>

</section>
</section>
<section>
<h1>Visualizing past, present and future</h1>

<p>
We can also plot our data in the same graph as our simulations, by extending the
axis of the <em>barplot</em> and using the <em>points</em> function:
</p>

<pre>
quantheatplot &lt;- function(x, sim, ylim) {
	q &lt;- 25
	simstart &lt;- length(x) - length(sim[1,])
	f &lt;- function(i) {
		if (i &lt; simstart)
			replicate(q+1, 0)
		else {
			qa &lt;- quantile(sim[,i-simstart], seq(0, 1, 1.0/q))
			c(qa[1], diff(qa))
		}
	}
	qvals &lt;- mapply(f, seq(1, length(x)))
	colors &lt;- c('white', colorsDouble(heat.colors, q))
	barplot(qvals, ylab='usd', xlab='day', col=colors, border=NA, space=0,
		names.arg=x, ylim=ylim)
	abline(h=totalspace, col='gray')
}
</pre>

<pre>
quantheatplot(c(day, seq(min(fday), max(fday), 'day')), dssim, ylim=c(min(c(usd, dssim)), max(dssim)))
points(usd)
abline(h=totalspace, col='gray')
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcspf1.png" alt=""> 
</p>

</section>
<section>
<h1>Training set, validation set</h1>

<p>
<a href="https://en.wikipedia.org/wiki/Cross-validation">Cross-validation</a> is
a technique that we can use to validate the use of Monte Carlo on our data.
</p>
<p>
We first split our data in two sets: the training set and the validation set. We
than use only the first in our simulations, and plot the second over. We can
then see graphically if the data fits our simulation.
</p>
<p>
Let's use the first two months as the training set, and the other three months
as the validation set:
</p>

<pre>
# Number of days to use in the training set
numdaysTrain &lt;- 60
numdaysVal   &lt;- length(day) - numdaysTrain

dssim2 &lt;- simulate(usd[seq(1, numdaysTrain)], numdaysVal-1)
</pre>

<pre>
allvals &lt;- c(usd, dssim2)
quantheatplot(day, dssim2, c(min(allvals), max(allvals)))
points(usd)
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcscv1.png" alt=""> 
</p>
<p>
Looks like using only the first two months already gives us a fair simulation.
What if we used only a single month, when no disk cleanup was performed?
</p>

<pre>
# Number of days to use in the training set
numdaysTrain &lt;- 30
numdaysVal   &lt;- length(day) - numdaysTrain

dssim3 &lt;- simulate(usd[seq(1, numdaysTrain)], numdaysVal-1)
</pre>

<pre>
allvals &lt;- c(usd, dssim3)
quantheatplot(day, dssim3, c(min(allvals), max(allvals)))
points(usd)
</pre>

<p>
 <img class="img-responsive" class="center" src="http://www.lpenz.org/articles/df0pred-3/mcscv2.png" alt=""> 
</p>
<p>
If we do regular disk cleanups, we must have at least one of them in our
training set to get realistic results. Our training set is not representative
without it.
</p>
<p>
This also tests our cross-validation code. A common mistake is using the
whole data set as the training set and as the validation set. That is not
cross-validation.
</p>

</section>
<section>
<h1>Conclusions</h1>

<p>
We can use Monte Carlo simulations not only to generate a distribution
probability of an event as we did in the <a href="http://www.lpenz.org/articles/df0pred-3/../df0pred-2/index.html">previous</a>
article, but also to predict a possible range of future values. In this article,
disk space occupation is not the most interesting example, as we are usually
more interested in knowing when our used disk space will reach a certain value
than in knowing the most probable values in time. But imagine that the data
represents the number of miles traveled in a road trip or race. You can then not
only see when you will arrive at your destination, but also the region where you
will probably be at any day.
</p>
<p>
There are plenty of other uses for this kind of prediction. Collect the data,
look at it and think if it would be useful to predict future ranges, and if it
makes sense with the data you have. Predictions based on the evidence can be
even used to support a decision or a point of view, just keep mind that
you can only use the past if you honestly don't think anything different is
going to happen.
</p>
</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>github project structure best practices</title>
			<link>http://www.lpenz.org/articles/github-project-struct</link>
			<guid>http://www.lpenz.org/articles/github-project-struct</guid>
			<pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
			<description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>These are my notes on the structural best practices I’ve been
following in my github repositories. We first go through the global
practices that apply to all kinds of projects, and then we cover the
language-dependent ones. This is definitely not supposed to be a
complete guide - rather, I’m documenting what I do once I have at least
2 repositories of a specific kind.</p>
<h1 id="global-practices">Global practices</h1>
<p>These are the practices that apply to almost all kinds of
repositories.</p>
<h2 id="workflow">Workflow</h2>
<p>Leave <em>main</em> as the default HEAD of the repository - that’s
the one that third parties get when they clone/check-out and the base of
pull requests.</p>
<p>Use a <em>devel</em> branch as a staging ground for commits. We can
then freely amend commits and push to this branch to make the complete
CI suite analyse it. Once a commit is good to go, just move the
<em>main</em> branch over to the latest commit with
<code>git push origin origin/devel:main</code>.</p>
<p>We can use other upstream branches to keep drafts of future work,
just be aware that they might start looking like <a
href="https://martinfowler.com/bliki/FeatureBranch.html">feature
branches</a>, which are not entirely ideal.</p>
<h2 id="files">Files</h2>
<p>A quick rundown of files that are expected in all repositories:</p>
<ul>
<li><code>README.md</code>: markdown file that’s rendered at the
project’s landing page.</li>
<li><code>LICENSE</code>: the software’s license. Yes, we need one in
every repository, if only to say that we are not liable for anything and
that what we’ve written is not fit for any specific purpose. I mostly
use MIT, which achieves both and doesn’t require a bit header in every
other file.</li>
<li><code>AUTHORS</code>: list of authors, required by some packaging
tools.</li>
<li><code>.gitignore</code>: files that should be ignored by git,
usually the files generated by the build system.</li>
<li><code>.gitattributes</code>: can be used to configure some files to
be ignored by <a href="https://git-scm.com/docs/git-archive">git
archive</a>, which is a command that generates a .tar.gz with the
contents of the repository. We should usually add the files that set up
CI, and any other files that don’t make sense outside of the
repository.</li>
<li><em>man page</em> for the command-line utilities. It can be
generated by the build system, though.</li>
</ul>
<h2 id="ci">CI</h2>
<h3 id="provider-github-actions">Provider: github actions</h3>
<p>Use a single <a href="https://github.com/features/actions">github
actions</a> workflow with as many jobs as necessary. That makes job
execution and dependencies easier to visualize and maintain than using
multiple workflows.</p>
<p>I have used <a href="https://www.travis-ci.com/">travis-ci</a> as a
best practice for almost 10 years before they <a
href="https://blog.travis-ci.com/2020-11-02-travis-ci-new-billing">changed
their pricing model</a> and added limits to public repositories. They
might have gone back on this, but they lost my trust.</p>
<h3 id="test-coverage">Test coverage</h3>
<p>Getting a 100% of tests passing is only meaningful if the tests are
good. One easy way to get a sense of how good the tests are is by
measuring test code coverage.</p>
<p>Use <a href="https://coveralls.io/">coveralls</a> for code coverage
reports. It has its quirks, but it can be made to work fine with just
about any language - and it supports build matrices natively.</p>
<p>I also used <em>codecov</em> in some more recent repositories. It
didn’t work very well with my <a href="#workflow">workflow</a> in the
past, but it seems to be working better than coveralls lately.</p>
<p>Add a test result and a coverage shield to the top of
<code>README.md</code>. That allows visitors to see that you project
cares about test coverage at a glance.</p>
<h3 id="formatter">Formatter</h3>
<p>Use a tool to check that files are well formatted - the stricter the
tool is, the better. This may sound a bit radical, but the truth is that
they end up making the code easier to work with for everybody, and
prevent silly conflicts.</p>
<p>Try to integrate the same tool in the code editor so that the
formatting doesn’t have to be fixed after the commit is pushed. Keeping
the format can otherwise become a chore.</p>
<h3 id="omnilint">omnilint</h3>
<p><a href="https://github.com/lpenz/omnilint">omnilint</a><sup><a
href="#disclaimer">disclaimer</a></sup> is a github action that performs
static analysis on standalone files. Add it as a first job in all
repositories:</p>
<pre class="{yml}"><code>jobs:
  omnilint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: docker://lpenz/omnilint:v0.2</code></pre>
<p>This makes sure that a minimum standard is kept even before the build
and tests are developed. It checks, for instance, that python files can
be loaded by the interpreter, without actually executing them (which
would require installing dependencies, etc.)</p>
<h3 id="buildtest-matrices">Build/test matrices</h3>
<p>github actions have build matrices support that we can use to test
the project in different environments - architectures and compilers.</p>
<p>Use matrices to test the environments where the project can be
actually deployed - there’s no need to go crazy and add all possible
combinations.</p>
<p>Use them also to test in a <em>future</em> or <em>nightly</em>
environment, so that we can get a heads up about upcoming language
changes.</p>
<h2 id="cd">CD</h2>
<h3 id="deploy-on-main-or-on-tagging">Deploy on <code>main</code> or on
tagging</h3>
<p>With regards to deployment, there are two options:</p>
<ul>
<li>deploy commits when they become the HEAD of the <code>main</code>
branch (branch-based);</li>
<li>deploy commits when they get tagged (tag-based).</li>
</ul>
<p>These are not mutually exclusive - we can actually use both in the
same project if it makes sense, separating them in different artifact
repositories. That allows users to what they want with regards to
stability versus frequency of updates.</p>
<p>Use <a href="https://semver.org/spec/v2.0.0.html">semver</a>, with
versions in the format <code>MAJOR.MINOR.PATCH</code>. The git tag is
usually the version prefixed by a <code>v</code>
(<code>vMAJOR.MINOR.PATCH</code>). The meaning of the numbers, according
to semver:</p>
<ul>
<li><em>MAJOR</em>: API-incompatible changes;</li>
<li><em>MINOR</em>: features;</li>
<li><em>PATCH</em>: bug fixes.</li>
</ul>
<p>This is the whole versioning scheme in tag-based deployments.</p>
<p>Branch-based deployments also use semver tags as checkpoints, but
they deploy using a version in the format
<code>MAJOR.MINOR.PATCH-DISTANCE</code>, where <code>DISTANCE</code> is
the number of commits since the tag <code>MAJOR.MINOR.PATCH</code>.</p>
<p>To generate the versions, use the <a
href="https://github.com/marketplace/actions/version-generator">version-generator</a>
<sup><a href="#disclaimer">disclaimer</a></sup> github action. It allows
us to simplify the deploy condition with something similar to:</p>
<pre class="{yml}"><code>(...)
      - id: version
        uses: docker://lpenz/ghaction-version-gen:0.3
      - name: deploy
        uses: &lt;deploy action&gt;
        if: steps.version.outputs.version_commit != &#39;&#39;
        with:
          version: #dollar#{{ steps.version.outputs.version_commit }}</code></pre>
<p>That deploys on commits. To deploy on tags, use
<code>version_tagged</code> instead of <code>version_commit</code>. The
action is also able to check if the tag matches the version in the
language-specific project file, for some languages.</p>
<h3 id="debian-on-packagecloud">Debian on packagecloud</h3>
<p>Package your projects in a format supported by distributions, such as
<code>.deb</code> or <code>.rpm</code> - that makes cross-language
dependencies manageable. Deploy the packages to a repository and use it
as an upstream source. <a
href="https://packagecloud.io/">packagecloud</a> is a very convenient
option for that, and it takes both formats mentioned.</p>
<p>Use the <a
href="https://github.com/marketplace/actions/deploy-to-packagecloud-io">Deploy
to packagecloud.io</a> <sup><a href="#disclaimer">disclaimer</a></sup>
github action, and add a corresponding shield to
<code>README.md</code>.</p>
<h3 id="nix">Nix</h3>
<p>TODO</p>
<h3 id="project-specific">Project-specific</h3>
<p>Also deploy to a specific location for the kind of project you
created:</p>
<ul>
<li><a href="https://crates.io/">crates</a> for rust</li>
<li><a href="https://pypi.org/">pypi</a> for python</li>
<li><a href="https://github.com/marketplace?type=actions">github
marketplace</a> for github actions</li>
</ul>
<p>This is specially important if the project is not a standalone
executable.</p>
<p>Most language-specific repositories provide a shield that should be
added to <code>README.md</code>.</p>
<h1 id="rust">Rust</h1>
<p><strong>Example</strong>: <a href="https://github.com/lpenz/ogle/"
class="uri">https://github.com/lpenz/ogle/</a></p>
<p>The rust ecosystem is at just the right point - it has most of the
problems already solved, and few solutions are already obsolete, making
the state-of-the-art easy to find.</p>
<p>We have proper github actions to run tests with coverage, the
formatter, static analyser (clippy), create .deb and language-specific
packages (crates). Use this github action workflow as a base: <a
href="https://github.com/lpenz/ogle/blob/main/.github/workflows/ci.yml"
class="uri">https://github.com/lpenz/ogle/blob/main/.github/workflows/ci.yml</a></p>
<p>Deploy to the language-specific repository at <a
href="https://crates.io/" class="uri">https://crates.io/</a>, and add
the corresponding shield to <code>README.md</code>.</p>
<p>Write a manpage if the project is a command line utility.</p>
<p>Some issues pending:</p>
<ul>
<li>man page generation: the man page is generated in a directory with a
randomly generated name, and we have to copy it manually before
generating the .deb.</li>
</ul>
<h1 id="cc">C/C++</h1>
<p><strong>Example</strong>: <a
href="https://github.com/lpenz/execpermfix/"
class="uri">https://github.com/lpenz/execpermfix/</a></p>
<p>C/C++ is on the other end of the spectrum: it’s a very old language,
older than even the concept of ecosystem. Every old problem has been
solved multiple times, and some new problems might not even be solved
yet - package management is the most important one.</p>
<p>That said, all the basics can be covered:</p>
<ul>
<li>Use <a
href="https://github.com/marketplace/actions/clang-format-lint">clang-format-lint</a>
for the format checking.</li>
<li>Use <a href="https://cmake.org/">cmake</a> as the build tool. I
honestly thing it’s a bad tool with way too many quirks and secret
incantations, but it’s the de facto standard, and it can do everything
when enough pressure is applied.</li>
<li>Use the <a
href="https://github.com/marketplace/actions/cmake-swiss-army-knife">cmake-swiss-army-knive</a>
<sup><a href="#disclaimer">disclaimer</a></sup> github action and get a
lot of different test types for free, as long as the
<code>CMakeLists.txt</code> file supports them.</li>
<li>Provide a basic man page for executables. It’s not hard to write one
by hand, the format doesn’t even allow it to get complex.</li>
<li>Provide <a
href="https://en.wikipedia.org/wiki/Pkg-config">pkg-config</a> files for
libraries. They can be used to pass down required compilation flags and
library dependencies to the users of the library, so that they don’t
have to figure those out and maintain them.</li>
</ul>
<p>We can use cmake to generate the install target and the .deb from
that.</p>
<p>Use this workflow as the base: <a
href="https://github.com/lpenz/execpermfix/blob/main/.github/workflows/ci.yml"
class="uri">https://github.com/lpenz/execpermfix/blob/main/.github/workflows/ci.yml</a></p>
<h1 id="python">Python</h1>
<p><strong>Example</strong>: <a
href="https://github.com/lpenz/ftpsmartsync/"
class="uri">https://github.com/lpenz/ftpsmartsync/</a></p>
<p>Python is known for having “batteries included”, but as another
language that’s been around for a while, those batteries had to be
changed several times. We have all the basics covered, including
packaging, but sometimes it’s not easy to find the <em>current</em> best
practice - not even on stackoverflow - due to answers that are plain
old.</p>
<p>Use python 3 - python 2’s end-of-life was on 2020-04-20.</p>
<p>Use <code>pyproject.toml</code> and <code>setup.cfg</code> instead of
<code>setup.py</code>. It’s not hard to make the conversion, as long as
there’s not a lot of dynamic generation. This is the current (very
superficial) official doc: <a
href="https://packaging.python.org/tutorials/packaging-projects/"
class="uri">https://packaging.python.org/tutorials/packaging-projects/</a></p>
<p>Use <code>stdeb</code> to create debian packages. It’s not perfect,
as there’s some friction with the way <code>share/doc</code> files are
deployed, but does the job.</p>
<p>Use the workflow at <a
href="https://github.com/lpenz/ftpsmartsync/blob/main/.github/workflows/ci.yml"
class="uri">https://github.com/lpenz/ftpsmartsync/blob/main/.github/workflows/ci.yml</a>
as a base.</p>
<h1 id="github-actions">github actions</h1>
<p><strong>Example</strong>: <a
href="https://github.com/lpenz/ghaction-cmake/"
class="uri">https://github.com/lpenz/ghaction-cmake/</a></p>
<p>We don’t have that many resources for github actions, to be
honest.</p>
<p>Create <em>docker</em> github actions if possible, as they are
self-contained, easier to test and tend to be faster.</p>
<p>Deploy the containers to <a href="https://hub.docker.com/"
class="uri">https://hub.docker.com/</a> using the <a
href="https://github.com/marketplace/actions/build-and-push-docker-images">docker/build-push-action</a>.
Docker Hub can automatically build containers, but it has a quota, and
using actions allows tests jobs to run before deployment.</p>
<p>To create a new version of a github action, tag the repository as
usual, let the container be deployed to docker hub and then use github’s
<code>Create a new release</code> link. This last step updates github’s
marketplace information, and can’t automated.</p>
<p>Add the following shields to <code>README.md</code>:</p>
<ul>
<li>marketplace link</li>
<li>github release</li>
<li>docker hub release</li>
</ul>
<p>That simplifies checking if the latest releases are in sync.</p>
<p>Note: don’t CD github actions from the <code>main</code> branch, as
we have the extra steps mentioned above.</p>
<h1 id="final-remarks">Final remarks</h1>
<p>This is probably the document that will get updated the most as we
figure out better ways to do things.</p>
<p>In short, we should strive to get the following whenever
possible:</p>
<ul>
<li>github actions</li>
<li>omnilint</li>
<li>automatic formatting</li>
<li>coverage with README.md shield</li>
<li>debian packaging and deployment from tags, with shield</li>
<li>language-specific deployment from tags (specially for libraries),
with shield</li>
</ul>
<p>The table below summarizes the language-specific tools that support
our scheme:</p>
<table class="table">
<thead>
<tr>
<th>
Category
</th>
<th>
Formatter
</th>
<th>
Debian
</th>
<th>
Deploy
</th>
<th>
Shields
</th>
<th>
Example
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="#rust">Rust</a>
</td>
<td>
<a href="https://crates.io/crates/rustfmt-nightly">rustfmt</a>
</td>
<td>
<a href="https://docs.rs/crate/cargo-deb/">cargo-deb</a>
</td>
<td>
<a href="https://packagecloud.io/">packagecloud.io</a>
</td>
<td>
<ul>
<li>coverage</li>
<li>crates.io release</li>
<li>packagecloud.io</li>
</ul>
</td>
<td>
<a
href="https://github.com/lpenz/ogle/blob/main/.github/workflows/ci.yml">ogle</a>
</td>
</tr>
<tr>
<td>
<a href="#cc">C/C++</a>
</td>
<td>
<a href="https://clang.llvm.org/docs/ClangFormat.html">clang-format</a>
</td>
<td>
<a
href="https://cmake.org/cmake/help/latest/manual/cpack.1.html">cpack</a>
</td>
<td>
<a href="https://packagecloud.io/">packagecloud.io</a>
</td>
<td>
<ul>
<li>coverage</li>
<li>packagecloud.io</li>
</ul>
</td>
<td>
<a
href="https://github.com/lpenz/execpermfix/blob/main/.github/workflows/ci.yml">execpermfix</a>
</td>
</tr>
<tr>
<td>
<a href="#python">Python</a>
</td>
<td>
<a href="https://pypi.org/project/black/">black</a>
</td>
<td>
<a href="https://pypi.org/project/stdeb/">stdeb</a>
</td>
<td>
<a href="https://packagecloud.io/">packagecloud.io</a>
</td>
<td>
<ul>
<li>coverage</li>
<li>packagecloud.io</li>
</ul>
</td>
<td>
<a
href="https://github.com/lpenz/ftpsmartsync/blob/main/.github/workflows/ci.yml">ftpsmartsync</a>
</td>
</tr>
<tr>
<td>
<a href="#github-actions">ghaction</a>
</td>
<td>
No
</td>
<td>
N/A
</td>
<td>
<a href="https://hub.docker.com/">Docker Hub</a>
</td>
<td>
<ul>
<li>marketplace</li>
<li>github release</li>
<li>dockerhub release</li>
</ul>
</td>
<td>
<a
href="https://github.com/lpenz/ghaction-cmake/blob/main/.github/workflows/ci.yml">ghaction-cmake</a>
</td>
</tr>
</tbody>
</table>
<h2 id="disclaimer">Disclaimer</h2>
<p>Be aware that I’ve tagged github actions that I created with a
<sup><a href="#disclaimer">disclaimer</a></sup> tag that links to this
section.</p>
<p>I had created some github actions before starting this document, and
I ended up creating a few more while write writing it. I avoided doing
that, though, but sometimes the sortcomings were too severe and/or very
easy to overcome.</p>
]]></description>
		</item>

	
		<item>
			<title>Using GPG and signing git commits</title>
			<link>http://www.lpenz.org/articles/gpg</link>
			<guid>http://www.lpenz.org/articles/gpg</guid>
			<pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
			<description><![CDATA[]]></description>
		</item>

	
		<item>
			<title>Haskell eDSL Tutorial - Shared expenses</title>
			<link>http://www.lpenz.org/articles/hedsl-sharedexpenses</link>
			<guid>http://www.lpenz.org/articles/hedsl-sharedexpenses</guid>
			<pubDate>Wed, 02 Feb 2011 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
People have created
<a href="http://ashish.typepad.com/ashishs_niti/2007/06/another_dsl_emb.html">interesting</a>
and
<a href="http://augustss.blogspot.com/2009/02/more-basic-not-that-anybody-should-care.html">weird</a>
embedded domain-specific languages in haskell. In this article, we will see what
an eDSL really is by building one to record shared expenses and calculate the
payments.
</p>
<p>
This article is written in literal-haskell style, so that you can simply paste it in a
<em>file.lhs</em> and then <em>runhaskell file.lhs</em> to run it. That's why we need the
lines bellow:
</p>

<pre>

&gt; import Data.Map as Map
&gt; import Control.Monad.State

</pre>

<section>
<h1>Why haskell</h1>

<p>
The first reason to use haskell for an eDSL is that it has a very clean syntax:
</p>

<ul>
<li>a function and its arguments are separated by spaces, not by parenthesis and
  commas - parenthesis are only used to make nested function calls;
</li>
<li>there is a syntax sugar for monads that let us avoid writing the monadic
  operators <em>&gt;&gt;</em> and <em>&gt;&gt;=</em> - it allows us to bind monads by using newlines;
</li>
<li>type inference allows us to skip type annotations and declarations, even
  though we are using a strongly typed language.
</li>
</ul>

<p>
When combined, these features allow us to leave very little haskell in our
eDSL, as we will see.
</p>

</section>
<section>
<h1>The shared expenses problem</h1>

<p>
Lets say that you went on a trip with 3 friends, and there are some costs that
are shared by everyone. You want to record these expenses and then everyone can
pay up once the trip is over.
</p>
<p>
In other words, you want records to look like this:
</p>

<pre>

&gt; trip = sharedexpenses $ do
&gt;     dexter `spent` 5300
&gt;     angel  `spent` 2700
&gt;     debra  `spent`  800
&gt;     harry  `spent` 1900
&gt;     debra  `spent` 1700
&gt;     angel  `spent` 2200

</pre>

<p>
Also, you want to be able to record transactions in which people lend money
straight to each other:
</p>

<pre>

&gt;     dexter `gave` harry $ 2000
&gt;     angel  `gave` debra $ 3200

</pre>

<p>
The haskell leaks we have in the records are the backticks (<code>`</code>) and the
<code>$</code>. We could get rid of them also, but the plumbing would get a lot more
convoluted. We also avoid using floating point numbers for a similar reason.
</p>

</section>
<section>
<h1>The state monad</h1>

<p>
By programming a new monad you get the "programmable semicolon" that people talk
so much about. That allows you to make a custom program flow different from
the standard top-down - it allows built-in backtracking, for instance.
</p>
<p>
But that is not an eDSL requirement. For our shared-expenses example, top-down
is just fine. The only thing we need is a way to store the expenses each person
had, and a simple
<a href="http://hackage.haskell.org/package/mtl/docs/Control-Monad-State.html">state monad</a>
with a
<a href="http://hackage.haskell.org/package/containers/docs/Data-Map.html">map</a>
inside can solve our problem.
</p>
<p>
In the next step we define what a person is and who our friends are:
</p>

<pre>

&gt; newtype Person = Person { name :: String } deriving (Eq, Ord, Show)

&gt; dexter = Person "Dexter"
&gt; angel  = Person "Angel"
&gt; debra  = Person "Debra"
&gt; harry  = Person "Harry"

</pre>

<p>
We could skip this step and just use strings here, but that would make typos a
runtime mistake; by using a strong type and defining the friends explicitly, we
make typos a compile error.
</p>

</section>
<section>
<h1>Spending and giving</h1>

<p>
<em>spent</em> and <em>gave</em> are functions that update our state:
</p>

<pre>

&gt; spent :: Person -&gt; Int -&gt; State (Map Person Int) ()
&gt; spent payer money = modify $ insertWith (+) payer money

&gt; gave :: Person -&gt; Person -&gt; Int -&gt; State (Map Person Int) ()
&gt; gave lender borrower money = modify $ (adjust (+ money) lender) . (adjust (\ m -&gt; m - money) borrower)

</pre>

<p>
<em>spent</em> adds the given amount to the element indexed by the person in the map,
while <em>gave</em> adds the amount to the lender and subtract it from the borrower.
</p>

</section>
<section>
<h1>Solving</h1>

<p>
To solve the shared expenses problem, we will use a simple algorithm: he who
owes more pays to the one that has more credit until everybody gets paid.
</p>

<pre>

&gt; solve st = solve' err $ Map.map ( \ m -&gt; m - avg) st
&gt;     where
&gt;         err = 1 + size st
&gt;         avg = round $ (toRational $ fold (+) 0 st) / (toRational $ size st)

&gt; solve' _   st | Map.null st = []
&gt; solve' err st =
&gt;     (name payer ++ " pays " ++ show amount ++ " to " ++ name receiver) : solve' err newstate
&gt;     where
&gt;         (payer,    debt)   = foldrWithKey (getpers True)  (Person "", 0) st
&gt;         (receiver, credit) = foldrWithKey (getpers False) (Person "", 0) st
&gt;         getpers True  p m (_,  m0) | m &lt; m0 = (p, m) -- Gets payer.
&gt;         getpers False p m (_,  m0) | m &gt; m0 = (p, m) -- Gets receiver.
&gt;         getpers _     _ _ e                 = e
&gt;         amount = min (-debt) credit
&gt;         newstate = Map.filter ( \ c -&gt; c &lt; -err || err &lt; c) $ mapWithKey statefix st
&gt;         statefix p m | p == receiver = m - amount
&gt;         statefix p m | p == payer = m + amount
&gt;         statefix _ m = m

</pre>

<p>
The <em>solve</em> functions subtracts from everybody the amount that each person is
supposed to spend (the average); the map now has the amount each person is
supposed to pay (negative) or receive (positive). When the amount in the map is
near 0, the person will not be involved in further transactions and is removed
from the map. "Near" here has a precise meaning: we take the number of persons
as the error (plus one), as we had to divide the total amount spent by it in
order to get the average spent - we will not be able to be more precise that
that using integers. Well, we are talking about money, it's useless to be more
precise than a cent anyway.
</p>
<p>
The <em>solve'</em> function recursively registers payments and removes persons from
the map until it is empty. That does not guarantee the least amount of payments,
but we get good enough results most of the times - and it is a lot simpler than
linear programming.
</p>

</section>
<section>
<h1>Plumbing</h1>

<p>
The function <code>sharedexpenses</code> is the one that glues the eDSL and the state
monad, while the <code>main</code> function is the one that plugs it with
the <code>solve</code> function and prints the results:
</p>

<pre>

&gt; sharedexpenses :: State (Map Person Int) () -&gt; Map Person Int
&gt; sharedexpenses f = execState f empty

&gt; main = mapM_ putStrLn $ solve trip

</pre>

<p>
Running this program provides us the following results:
</p>

<pre>
Debra pays 4350 to Angel
Harry pays 3650 to Dexter
Harry pays 100 to Angel
</pre>

</section>
<section>
<h1>Conclusions</h1>

<p>
We have seen what is an eDSL by building a solution to a real, day-to-day
problem. No hidden enchantments or dark arts involved: you don't have to build a
custom monad or start with something that looks like
<em>data Expr a = ...</em>; you can just define how you want your language to look
like, think about what the state needs to store and build the plumbing around
the state monad - or even around the
<a href="http://hackage.haskell.org/package/mtl/docs/Control-Monad-Writer.html">writer monad</a>.
You can also use nested state monads to define heterogeneous environments with
different "syntaxes" verified by the type system. The only drawback is that
every user of your language will need a haskell compiler installed, but with
the <a href="http://www.haskell.org/platform/">haskell platform</a> available, that
shouldn't be a problem.
</p>

</section>
<section>
<h1>Further reading</h1>

<ul>
<li><a href="http://paulspontifications.blogspot.com.br/2008/01/why-haskell-is-good-for-embedded-domain.html">http://paulspontifications.blogspot.com.br/2008/01/why-haskell-is-good-for-embedded-domain.html</a>:
  Why Haskell is Good for Embedded Domain Specific Languages
</li>
<li><a href="https://donsbot.wordpress.com/2007/03/10/practical-haskell-shell-scripting-with-error-handling-and-privilege-separation/">https://donsbot.wordpress.com/2007/03/10/practical-haskell-shell-scripting-with-error-handling-and-privilege-separation/</a>:
  Practical Haskell: shell scripting with error handling and privilege separation
</li>
<li><a href="http://gbacon.blogspot.com/2009/07/programmable-semicolon-explained.html">http://gbacon.blogspot.com/2009/07/programmable-semicolon-explained.html</a>:
  Programmable semicolon explained
</li>
</ul>

</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Create a debian bootable live USB</title>
			<link>http://www.lpenz.org/articles/liveusb</link>
			<guid>http://www.lpenz.org/articles/liveusb</guid>
			<pubDate>Sun, 04 Oct 2015 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
<strong>Updated 2019-06-25</strong>: use Debian Stretch explicitly (already implied
by kernel version); suggest testing with qemu.
</p>
<p>
We can install a full Linux distribution in a USB drive, and use it to boot a
system. That is called a "live USB," and it can be used for recovery, as a
portable environment, etc.
</p>
<p>
In this article we explain how to install a Debian GNU/Linux OS in a USB drive
as if it was a hard disk. We will use Debian's own <em>debootstrap</em> to populate
the root partition and <em>syslinux</em> as a bootloader (it is simpler than the
standard <em>grub</em>).
</p>
<p>
Obs: to ease copy-and-pasting, we show the commands without prompt, and
prepend <code>&gt;</code> to the output of commands on most examples.
</p>

<section>
<h1>1 Partitioning</h1>

<p>
After inserting the USB drive, it will appear as a block device under <em>/dev</em>,
usually <em>sd[a-z]</em>. Take a note on the device name. We will use <em>/dev/sdc</em>
through the examples.
</p>
<p>
Create two partitions on our USB drive: one with 256MB for the <em>/boot</em> that
will hold the <em>syslinux</em> bootloader and the Linux kernel; and another with all
the rest of the space, that will hold the root filesystem.
</p>
<p>
There are many utilities you can use to partition the USB drive: <em>parted</em>,
<em>fdisk</em>, etc.
</p>
<p>
For example, using fdisk, run it on <em>/dev/sdc</em>:
</p>

<pre>
fdisk /dev/sdc
&gt; Welcome to fdisk (util-linux 2.25.2).
&gt; Changes will remain in memory only, until you decide to write them.
&gt; Be careful before using the write command.
</pre>

<p>
You are left at <em>fdisk</em>'s command prompt. Create the boot partition:
</p>

<pre>
Command (m for help): n
&gt; Partition type
&gt;    p   primary (0 primary, 0 extended, 4 free)
&gt;    e   extended (container for logical partitions)
&gt; Select (default p): p
&gt; Partition number (1-4, default 1): 1
&gt; First sector (2048-31350782, default 2048):
&gt; Last sector, +sectors or +size{K,M,G,T,P} (2048-31350782, default 31350782): +256M
&gt; 
&gt; Created a new partition 1 of type 'Linux' and of size 256 MiB.
</pre>

<p>
Set its type to <em>FAT16</em>:
</p>

<pre>
Command (m for help): t
&gt; Selected partition 1
&gt; Hex code (type L to list all codes): 6
&gt; If you have created or modified any DOS 6.x partitions, please see the fdisk documentation for additional information.
&gt; Changed type of partition 'Linux' to 'FAT16'.
</pre>

<p>
Mark it as <em>active</em>:
</p>

<pre>
Command (m for help): a
&gt; The bootable flag on partition 1 is enabled now.
</pre>

<p>
Create the root partition:
</p>

<pre>
Command (m for help): n
&gt; Partition type
&gt;    p   primary (1 primary, 0 extended, 3 free)
&gt;    e   extended (container for logical partitions)
&gt; Select (default p): p
&gt; Partition number (2-4, default 2): 2
&gt; First sector (526336-31350782, default 526336):
&gt; Last sector, +sectors or +size{K,M,G,T,P} (526336-31350782, default 31350782):
&gt; 
&gt; Created a new partition 2 of type 'Linux' and of size 14.7 GiB.
</pre>

<p>
Check that they were created:
</p>

<pre>
Command (m for help): p
&gt; Disk /dev/sdc: 15 GiB, 16051600896 bytes, 31350783 sectors
&gt; Units: sectors of 1 * 512 = 512 bytes
&gt; Sector size (logical/physical): 512 bytes / 512 bytes
&gt; I/O size (minimum/optimal): 512 bytes / 512 bytes
&gt; Disklabel type: dos
&gt; Disk identifier: 0x13090bb3
&gt; 
&gt; Device     Boot  Start      End  Sectors  Size Id Type
&gt; /dev/sdc1  *      2048   526335   524288  256M  6 FAT16
&gt; /dev/sdc2       526336 31350782 30824447 14.7G 83 Linux
</pre>

<p>
Save and exit:
</p>

<pre>
Command (m for help): w
&gt; The partition table has been altered.
&gt; Calling ioctl() to re-read partition table.
&gt; Syncing disks.
</pre>

<p>
We now have a <em>/dev/sdc1</em> that will be our <em>/boot</em>, and <em>/dev/sdc2</em> that
will be our root file system. Observe that the boot partition has a MS-DOS
type - that is required <em>syslinux</em>.
</p>
<p>
(the instructions above are heavily based on
<a href="http://allskyee.blogspot.com.br/2014/01/using-syslinux-to-boot-debootstraped.html">using-syslinux-to-boot-debootstraped</a>)
</p>

</section>
<section>
<h1>2 Installing the bootloader</h1>

<p>
Create a FAT16 filesystem on the boot device:
</p>

<pre>
mkdosfs -n LINUXBOOT /dev/sdc1
&gt; mkfs.fat 3.0.27 (2014-11-12)
</pre>

<p>
Install <em>syslinux</em> on it:
</p>

<pre>
syslinux /dev/sdc1
</pre>

</section>
<section>
<h1>3 Installing the base system in the root partition</h1>

<p>
Create the filesystem:
</p>

<pre>
mkfs.ext4 /dev/sdc2
&gt; mke2fs 1.42.12 (29-Aug-2014)
&gt; Creating filesystem with 3853055 4k blocks and 964768 inodes
&gt; Filesystem UUID: 68d66fd5-97f2-46ed-aee6-dad6f228a172
&gt; Superblock backups stored on blocks:
&gt;         32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208
&gt; 
&gt; Allocating group tables: done
&gt; Writing inode tables: done
&gt; Creating journal (32768 blocks): done
&gt; Writing superblocks and filesystem accounting information: done
</pre>

<p>
You can use any filesystem here, as long as it is supported by your future
kernel.
</p>
<p>
Mount both partitions and use <em>debootstrap</em> to install the base files on it:
</p>

<pre>
mkdir -p usbroot
mount -t auto /dev/sdc2 usbroot
mkdir -p usbroot/boot
mount -t auto /dev/sdc1 usbroot/boot
debootstrap stretch usbroot http://ftp.debian.org/debian
&gt; I: Retrieving Release.gpg 
&gt; I: Checking Release signature
&gt; I: Valid Release signature (key id 75DDC3C4A499F1A18CB5F3C8CBF8D6FD518E17E1)
&gt; I: Retrieving Packages 
&gt; I: Validating Packages 
&gt; I: Resolving dependencies of required packages...
&gt; I: Resolving dependencies of base packages...
&gt; I: Found additional required dependencies: acl adduser dmsetup insserv libaudit1 libaudit-common libbz2-1.0 libcap2 libcap2-bin libcryptsetup4 libdb5.3 libdebconfclient0 libdevmapper1.02.1 libgcrypt20 libgpg-error0 libkmod2 libncursesw5 libprocps3 libsemanage1 libsemanage-common libslang2 libsystemd0 libudev1 libustr-1.0-1 procps systemd systemd-sysv udev 
&gt; I: Found additional base dependencies: libdns-export100 libffi6 libgmp10 libgnutls-deb0-28 libgnutls-openssl27 libhogweed2 libicu52 libidn11 libirs-export91 libisccfg-export90 libisc-export95 libmnl0 libnetfilter-acct1 libnettle4 libnfnetlink0 libp11-kit0 libpsl0 libtasn1-6 
&gt; I: Checking component main on http://ftp.debian.org/debian...
&gt; I: Retrieving acl 2.2.52-2
&gt; (...)
&gt; I: Configuring libc-bin...
&gt; I: Configuring systemd...
&gt; I: Base system installed successfully.
</pre>

</section>
<section>
<h1>4 On-root configuration</h1>

<p>
We will have to <em>chroot</em> into our root filesystem to configure it further.
</p>
<p>
Mount the boot device and the default ones inside the root mount point:
</p>

<pre>
mount -t devtmpfs dev       usbroot/dev
mount -t devpts   devpts    usbroot/dev/pts
mount -t proc     proc      usbroot/proc
mount -t sysfs    sysfs     usbroot/sys
</pre>

<p>
<em>chroot</em> into root:
</p>

<pre>
chroot usbroot /bin/bash
</pre>

<p>
Set the root user password:
</p>

<pre>
passwd
&gt; Enter new UNIX password:
&gt; Retype new UNIX password:
&gt; passwd: password updated successfully
</pre>

<p>
Install the Linux kernel and other important packages:
</p>

<pre>
apt-get install --no-install-recommends -y linux-image-amd64 syslinux busybox-static
&gt; Reading package lists... Done
&gt; Building dependency tree... Done
&gt; The following extra packages will be installed:
&gt;   initramfs-tools klibc-utils libklibc libuuid-perl linux-base linux-image-4.9.0-9-amd64
&gt; Suggested packages:
&gt;   bash-completion linux-doc-3.16 debian-kernel-handbook grub-pc grub-efi extlinux
&gt; Recommended packages:
&gt;   busybox busybox-initramfs busybox-static firmware-linux-free irqbalance
&gt; The following NEW packages will be installed:
&gt;   initramfs-tools klibc-utils libklibc libuuid-perl linux-base linux-image-4.9.0-9-amd64 linux-image-amd64
&gt; 0 upgraded, 7 newly installed, 0 to remove and 0 not upgraded.
&gt; Need to get 34.1 MB of archives.
&gt; After this operation, 164 MB of additional disk space will be used.
&gt; (...)
&gt; Setting up linux-image-amd64 (3.16+63) ...
&gt; Processing triggers for initramfs-tools (0.120) ...
&gt; ln: failed to create hard link '/boot/initrd.img-4.9.0-9-amd64.dpkg-bak' =&gt; '/boot/initrd.img-4.9.0-9-amd64': Operation not permitted
&gt; update-initramfs: Generating /boot/initrd.img-4.9.0-9-amd64
</pre>

<p>
We now have to set up our mount points in the <em>/etc/fstab</em> of the USB drive,
but if we simply use <em>/dev/sdc*</em> as the devices, we will have trouble mounting
it on other systems with a different number of hard drives. To have stable mount
points, we use the <em>UUID</em> - universal unique identifiers - of the filesystems.
Use <em>blkid</em> to find out the values of your identifiers:
</p>

<pre>
blkid
&gt; (...)
&gt; /dev/sdc1: SEC_TYPE="msdos" UUID="2420-26B1" TYPE="vfat" PARTUUID="13090bb3-01"
&gt; /dev/sdc2: UUID="68d66fd5-97f2-46ed-aee6-dad6f228a172" TYPE="ext4" PARTUUID="13090bb3-02"
</pre>

<p>
In this example, the <em>UUID</em> of the boot filesystem is <code>2420-26B1</code>, and the
<em>UUID</em> of the root filesystem is <code>68d66fd5-97f2-46ed-aee6-dad6f228a172</code>. Use
them to populate <em>/etc/fstab</em>:
</p>

<pre>
echo 'UUID=68d66fd5-97f2-46ed-aee6-dad6f228a172 /     ext4 defaults,noatime 0 0' &gt;  etc/fstab
echo 'UUID=2420-26B1                            /boot vfat defaults         0 0' &gt;&gt; etc/fstab
</pre>

<p>
Figure out the name of the kernel and initrd installed on the boot partition:
</p>

<pre>
ls boot/vmlinuz* boot/initrd*
&gt; boot/initrd.img-4.9.0-9-amd64  boot/vmlinuz-4.9.0-9-amd64
</pre>

<p>
And use them with the <em>UUID</em>s to create the <code>boot/syslinux.cfg</code> file, with
the following contents:
</p>

<pre>
default linux
timeout 1
prompt 1

label linux
    kernel vmlinuz-4.9.0-9-amd64
    append initrd=initrd.img-4.9.0-9-amd64 root=UUID=68d66fd5-97f2-46ed-aee6-dad6f228a172 ro
</pre>

<p>
Finally, write <em>syslinux</em>'s master boot record on the USB drive:
</p>

<pre>
cat /usr/lib/SYSLINUX/mbr.bin &gt; /dev/sdc
</pre>

</section>
<section>
<h1>5 Closing up</h1>

<p>
We are now ready to leave the <em>chroot</em> and umount all devices:
</p>

<pre>
exit
umount usbroot/dev/pts
umount usbroot/dev
umount usbroot/proc
umount usbroot/sys
umount usbroot/boot
umount usbroot
</pre>

<p>
We can test our system using qemu:
</p>

<pre>
qemu-system-x86_64 -m 512 -hda /dev/sdc
</pre>

<p>
We should be able to login as <em>root</em>, with the password we set above.
</p>
<p>
If everything is working as expected, we can now remove the USB drive
and use it to boot any computer.
</p>

</section>
<section>
<h1>References</h1>

<p>
This article is, in fact, basically a rehash of the following references with
the <em>UUID</em> part added.
</p>

<ul>
<li><a href="http://allskyee.blogspot.com.br/2014/01/using-syslinux-to-boot-debootstraped.html">http://allskyee.blogspot.com.br/2014/01/using-syslinux-to-boot-debootstraped.html</a>
</li>
<li><a href="http://quietsche-entchen.de/cgi-bin/wiki.cgi/ariane/BootableUsbStick">http://quietsche-entchen.de/cgi-bin/wiki.cgi/ariane/BootableUsbStick</a>
</li>
</ul>

<p>
I've also created a
<a href="https://gist.github.com/lpenz/e7339a0b309e29698186baee92370104">gist</a>
that's easy to change, with the commands in this article.
</p>
</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Creating an encrypted directory-in-a-file</title>
			<link>http://www.lpenz.org/articles/luksfile</link>
			<guid>http://www.lpenz.org/articles/luksfile</guid>
			<pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
			<description><![CDATA[]]></description>
		</item>

	
		<item>
			<title>Creating a static nix channel</title>
			<link>http://www.lpenz.org/articles/nixchannel</link>
			<guid>http://www.lpenz.org/articles/nixchannel</guid>
			<pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
			<description><![CDATA[<div class="body" id="body">
<p>
TL;DR: full example at <a href="https://github.com/lpenz/nixpkgs-lpenz/">https://github.com/lpenz/nixpkgs-lpenz/</a>
</p>
<p>
<a href="https://nixos.org">Nix</a> is a packet manager for Unix-like systems that
can be effectively used as an overlay distribution.
</p>
<p>
These are my notes on how to create a static nix channel with a binary
cache that can be used to distribute compiled packages - and how to do
it using <a href="https://travis-ci.com">travis-ci</a> and
<a href="https://pages.github.com/">github pages</a>. This assumes that we already
have the derivations that we want to distribute.
</p>
<p>
You should also check out <a href="https://cachix.org/">Cachix</a>, as an
alternative to what is described here.
</p>

<section>
<h1>Generating key pair</h1>

<p>
The first step in creating a repository is generating a key pair:
</p>

<pre>
nix-store --generate-binary-cache-key lpenz.org cache-priv-key.pem cache-pub-key.pem
</pre>

<p>
That creates the single-line files <code>cache-priv-key.pem</code> and
<code>cache-pub-key.pem</code>. The first has the private key, which we will
use to do the packaging process in travis-ci (store it in a safe
place). The second file has the public key that we will use to
authenticate the packages in the clients.
</p>

</section>
<section>
<h1>Setting up the repository in github and travis-ci</h1>

<p>
This article is not going into the details of how to create a repository and populate it with nix derivations. Here are some references on that:
</p>

<ul>
<li><a href="https://nixos.org/nixos/nix-pills/generic-builders.html">https://nixos.org/nixos/nix-pills/generic-builders.html</a>
</li>
<li><a href="https://nixos.org/nixpkgs/manual/#chap-stdenv">https://nixos.org/nixpkgs/manual/#chap-stdenv</a>
</li>
</ul>

<p>
To set up github pages, go to the <em>Settings</em> page of the github
repsitory with the derivations, <em>GitHub Pages</em> and set <em>Source</em> to
the <em>gh-pages branch</em>.
</p>
<p>
To set up travis-ci, create a <em>NIX_CACHE_PRIV_KEY</em> environment variable in
the settings
(<a href="https://docs.travis-ci.com/user/environment-variables/">details</a>),
with the contents of <code>cache-priv-key.pem</code>. Keep in mind the best
practices at
<a href="https://docs.travis-ci.com/user/best-practices-security#recommendations-on-how-to-avoid-leaking-secrets-to-build-logs">Recommendations on how to avoid leaking secrets to build logs</a>
to avoid problems.
</p>

<section>
<h2>Creating the channel</h2>

<p>
A channel is simply a <code>nixexprs.tar.xz</code> with all nix derivation
files, and a <code>binary-cache-url</code> file that, as the name implies,
points to the binary cache URL.
</p>
<p>
Let's assume that we are creating the channel in the <code>_output</code>
directory, and that we will put the cache files in
<code>_output/cache</code>. To create the <code>nixexprs.tar.xz</code> file:
</p>

<pre>
tar -cJf _output/nixexprs.tar.xz ./*.nix \
    --transform "s,^,${PWD##*/}/," \
    --owner=0 --group=0 --mtime="1970-01-01 00:00:00 UTC"
</pre>

<p>
Creating the <code>binary-cache-url</code> file is simpler:
</p>

<pre>
echo '&lt;URL of cache&gt;' &gt; _output/binary-cache-url
</pre>

<p>
Nix also tries to retrieve the channel URL by itself - create a
<code>index.html</code> file to handle that:
</p>

<pre>
touch _output/index.html
</pre>

</section>
<section>
<h2>Creating the binary cache</h2>

<p>
To create the binary cache, start by building the packages with:
</p>

<pre>
nix-build
</pre>

<p>
Then sign the results:
</p>

<pre>
export NIX_SECRET_KEY_FILE="$PWD/nix-cache-priv-key.pem"
echo "$NIX_CACHE_PRIV_KEY" &gt; "$NIX_SECRET_KEY_FILE"
nix store sign -k "$NIX_SECRET_KEY_FILE"
</pre>

<p>
And finally, copy the results from the store to the cache directory:
</p>

<pre>
nix copy --to "file:///$PWD/_output/cache"
</pre>

</section>
<section>
<h2>.travis.yml deploy section</h2>

<p>
Detailed instructions on how to set this up are at
<a href="https://docs.travis-ci.com/user/deployment/pages/">GitHub Pages Deployment</a>.
We essentially have to set up a GITHUB_TOKEN.
</p>
<p>
The deploy section of <code>.travis.yml</code> ends up looking like the following:
</p>

<pre>
deploy:
  provider: pages
  skip-cleanup: true
  github-token: "$GITHUB_TOKEN"
  local_dir: _output
  target-branch: gh-pages
  on:
    branch: master
</pre>

</section>
</section>
<section>
<h1>Using the channel</h1>

<p>
To use the channel, add the channel URL to nix:
</p>

<pre>
nix-channel --add &lt;URL of channel&gt;
nix-channel --update
</pre>

<p>
That allows us to build packages from the channel, but it doesn't
make nix use the binary cache. To enable the cache, we have to set up
the channel as a <em>substituter</em>. To do that, put the following in
<code>~/.config/nix/nix.conf</code>, creating the file it if it doesn't exist:
</p>

<pre>
substituters = https://cache.nixos.org &lt;URL of cache&gt;
trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= &lt;cache-pub-key.pem contents&gt;
</pre>

<p>
Note that we have to also add the default, as that line is an absolute
configuration.
</p>

</section>
<section>
<h1>Conclusions</h1>

<p>
Nix is very versatile, and one way to use it is an overlay
distribution, by installing <a href="https://github.com/NixOS/nixpkgs">nixpkgs</a>
over an existing non-NixOS linux installation. On that setup, a
private nix channel can be very useful to develop and deploy software
consistently without being pegged to a particular gloal distribution
or environment - and the binary cache allows us to do that without
even having to recompile from source. That's how I've been using it,
and I'm quite happy with the results.
</p>
</section>
</div>
]]></description>
		</item>

	
		<item>
			<title>Rust snippets</title>
			<link>http://www.lpenz.org/articles/rust-snippets</link>
			<guid>http://www.lpenz.org/articles/rust-snippets</guid>
			<pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate>
			<description><![CDATA[]]></description>
		</item>


    </channel>
</rss>
